name: Darwin Workflow - Health Check

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'darwin-workflow/**'
      - 'init.sh'
      - 'setup.sh'
      - 'start.sh'
      - 'services.yaml'
  workflow_dispatch:  # Allow manual triggering

jobs:
  health-check:
    runs-on: [self-hosted, Linux, X64, darwin]
    defaults:
      run:
        working-directory: .

    steps:
      - name: Runner identity (debug)
        run: |
          echo "Runner name: $RUNNER_NAME"
          echo "Runner OS: $RUNNER_OS"
          echo "Runner arch: $RUNNER_ARCH"
          hostname
      - name: Cleanup previous run artifacts
        run: |
          echo "ðŸ§¹ Cleaning up previous run artifacts..."
          # Clean up kind cluster if it exists
          if command -v kind >/dev/null 2>&1; then
            if kind get clusters 2>/dev/null | grep -q kind; then
              echo "Deleting existing kind cluster..."
              kind delete cluster --name kind || true
            fi
          fi
          # Clean up any leftover directories that might cause permission issues during checkout
          # The checkout action fails if it can't remove directories from previous runs
          WORKSPACE_DIR="${{ github.workspace }}"
          if [ -d "$WORKSPACE_DIR" ]; then
            echo "Cleaning up workspace directory: $WORKSPACE_DIR"
            # Try to remove kind/shared-storage directories that might have permission issues
            # These are often created by Kubernetes volumes and may have restrictive permissions
            if [ -d "$WORKSPACE_DIR/kind" ]; then
              echo "Cleaning up kind directory..."
              sudo chmod -R u+w "$WORKSPACE_DIR/kind" 2>/dev/null || true
              sudo rm -rf "$WORKSPACE_DIR/kind/shared-storage" 2>/dev/null || true
              find "$WORKSPACE_DIR/kind" -type d -name "shared-storage" -exec sudo rm -rf {} + 2>/dev/null || true
            fi
          fi
          echo "âœ… Cleanup completed"
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive
          clean: true
          fetch-depth: 1

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Clean up disk space
        run: |
          echo "ðŸ§¹ Cleaning up disk space before build..."
          echo "Current disk usage:"
          df -h
          echo ""
          echo "Cleaning up Docker resources..."
          docker system prune -a -f || true
          docker volume prune -f || true
          echo ""
          echo "Cleaning up apt cache..."
          sudo apt-get clean || true
          sudo rm -rf /var/lib/apt/lists/* || true
          echo ""
          echo "Disk usage after cleanup:"
          df -h
      - name: Install prerequisites
        run: |
          # Install kubectl
          KUBECTL_VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
          echo "Installing kubectl version: $KUBECTL_VERSION"
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          # Verify the download is actually a binary (not an error page)
          if ! file kubectl | grep -q "ELF\|executable"; then
            echo "âŒ Downloaded file is not a binary, retrying with direct URL..."
            rm -f kubectl
            curl -LO "https://storage.googleapis.com/kubernetes-release/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          fi
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client
          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version
          # Install Kind
          curl -Lo /tmp/kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
          chmod +x /tmp/kind
          sudo mv /tmp/kind /usr/local/bin/kind
          kind version
          # Install yq (required by init.sh)
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          yq --version
      - name: Initialize submodules
        run: |
          git submodule sync --recursive
          git submodule update --init --recursive --remote || true
      - name: Step 1 - Initialize configuration (init.sh)
        run: |
          echo "ðŸ“‹ Step 1: Running init.sh to configure services..."
          
          # Clean up any existing config to avoid overwrite prompt
          rm -f .setup/enabled-services.yaml
          
          # Answers (13 prompts total):
          # 1-7: compute, workspace, feature_store, mlflow, serve, catalog, chronos (all n)
          # 8: workflow (y) - will auto-enable darwin-compute and darwin-cluster-manager as dependencies
          # 9: confirm (y)
          # 10: darwin sdk runtime (n)
          # 11: ray:2.37.0 (n)
          # 12: ray:2.53.0 (n)
          # 13: hermes-cli (n)
          # Use printf with explicit newlines and ensure stdin is properly closed
          printf "n\nn\nn\nn\nn\nn\nn\ny\ny\nn\nn\nn\nn\n" | ./init.sh || {
            echo "âŒ init.sh failed with exit code $?"
            echo "Checking if config was created..."
            if [ -f .setup/enabled-services.yaml ]; then
              echo "Config file exists, showing contents:"
              cat .setup/enabled-services.yaml
            else
              echo "Config file was not created"
            fi
            exit 1
          }
          
          echo "âœ… Configuration created by init.sh"
          cat .setup/enabled-services.yaml
      - name: Step 2 - Build images and setup cluster (setup.sh)
        run: |
          echo "ðŸ”¨ Step 2: Running setup.sh to build images and setup cluster..."
          # Clean up Docker before building to free space (GitHub Actions runners have limited disk space)
          echo "ðŸ§¹ Cleaning up Docker before build..."
          docker system prune -f || true
          docker image prune -f || true
          echo "Disk space before build:"
          df -h
          # Ensure ENV is set to local for cluster creation
          export ENV=local
          # Run setup.sh with auto-yes flag (it will auto-answer yes to cluster setup and clean build)
          # setup.sh will handle errors gracefully - if cluster creation fails but KUBECONFIG exists, it will continue
          ./setup.sh -y
          
          # Verify config.env was created (setup.sh should create it at project root)
          echo "ðŸ” Verifying config.env was created..."
          if [ -f config.env ]; then
            echo "âœ… config.env exists"
            echo "Contents:"
            cat config.env
          else
            echo "âŒ config.env not found! setup.sh may have failed."
            echo "Current directory: $(pwd)"
            echo "Looking for config.env..."
            find . -name "config.env" -type f 2>/dev/null | head -5 || echo "Not found"
            exit 1
          fi
          
          # Clean up unused images after build to free space for deployment
          echo "ðŸ§¹ Cleaning up unused Docker images after build..."
          docker image prune -f || true
          echo "Disk space after build:"
          df -h
      - name: Verify cluster is ready
        run: |
          echo "ðŸ” Verifying cluster status..."
          export KUBECONFIG=./kind/config/kindkubeconfig.yaml
          
          if [ ! -f "$KUBECONFIG" ]; then
            echo "âŒ KUBECONFIG not found, cluster may not be ready"
            exit 1
          fi
          
          # Wait for cluster to be ready
          max_attempts=30
          attempt=0
          while [ $attempt -lt $max_attempts ]; do
            if kubectl get nodes >/dev/null 2>&1; then
              echo "âœ… Cluster is accessible"
              kubectl get nodes
              break
            fi
            echo "Attempt $((attempt+1))/$max_attempts: Waiting for cluster..."
            sleep 5
            attempt=$((attempt+1))
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo "âŒ Cluster is not accessible"
            exit 1
          fi
      - name: Step 3 - Deploy to Kubernetes (start.sh)
        run: |
          echo "ðŸš€ Step 3: Running start.sh to deploy to Kubernetes..."
          export KUBECONFIG=./kind/config/kindkubeconfig.yaml
          
          # Verify config.env exists before running start.sh
          echo "ðŸ” Pre-flight checks..."
          echo "Current directory: $(pwd)"
          echo "Checking for config.env..."
          if [ -f config.env ]; then
            echo "âœ… config.env exists at: $(realpath config.env)"
            echo "Contents:"
            cat config.env
          else
            echo "âŒ config.env not found in current directory!"
            echo "Searching for config.env..."
            find . -name "config.env" -type f 2>/dev/null | head -5 || echo "Not found anywhere"
            exit 1
          fi
          
          # Verify cluster is accessible before deployment
          echo "ðŸ” Verifying cluster connectivity..."
          if ! kubectl get nodes >/dev/null 2>&1; then
            echo "âŒ Cluster is not accessible"
            exit 1
          fi
          
          # Run start.sh - it will handle config.env sourcing internally
          # start.sh now uses absolute paths and will provide clear error if config.env is missing
          echo "ðŸ“‹ Running start.sh..."
          
          # Start a background process to periodically show status and keep runner alive
          (
            while true; do
              sleep 30
              echo ""
              echo "â³ [$(date +%H:%M:%S)] Deployment in progress... Showing current status:"
              echo "   Helm releases:"
              helm list -n darwin 2>/dev/null | head -3 || echo "      (no releases yet)"
              echo "   Pod status:"
              kubectl get pods -n darwin --no-headers 2>/dev/null | head -5 | awk '{print "      " $1 ": " $3}' || echo "      (no pods yet)"
              echo ""
            done
          ) &
          STATUS_MONITOR_PID=$!
          
          # Capture output and exit code
          set +e  # Don't exit on error immediately, we want to capture diagnostics
          ./start.sh 2>&1 | tee /tmp/start.sh.log
          START_EXIT_CODE=${PIPESTATUS[0]}
          set -e
          
          # Stop the status monitor
          kill $STATUS_MONITOR_PID 2>/dev/null || true
          
          if [ $START_EXIT_CODE -ne 0 ]; then
            echo "âŒ start.sh failed with exit code $START_EXIT_CODE"
            echo ""
            echo "=== start.sh output ==="
            cat /tmp/start.sh.log || true
            echo ""
            echo "=== Helm releases ==="
            helm list -n darwin || true
            echo ""
            echo "=== Helm release status ==="
            helm status darwin -n darwin || true
            echo ""
            echo "=== Helm release history ==="
            helm history darwin -n darwin || true
            echo ""
            echo "=== All pods in darwin namespace ==="
            kubectl get pods -n darwin || true
            echo ""
            echo "=== Failed pods details ==="
            kubectl get pods -n darwin -o json | jq -r '.items[] | select(.status.phase != "Running" and .status.phase != "Succeeded") | "\(.metadata.name): \(.status.phase) - \(.status.reason // "N/A")"' || true
            echo ""
            echo "=== Pod events ==="
            kubectl get events -n darwin --sort-by='.lastTimestamp' | tail -20 || true
            echo ""
            echo "=== Readiness check job (if exists) ==="
            kubectl get jobs -n darwin | grep readiness || true
            kubectl logs -n darwin -l job-name=darwin-services-readiness-check --tail=50 || true
            echo ""
            echo "=== All resources in darwin namespace ==="
            kubectl get all -n darwin || true
            echo ""
            echo "=== Describe failed pods ==="
            for pod in $(kubectl get pods -n darwin -o jsonpath='{.items[?(@.status.phase!="Running" && @.status.phase!="Succeeded")].metadata.name}'); do
              echo "--- Describing pod: $pod ---"
              kubectl describe pod -n darwin "$pod" || true
              echo "--- Logs for pod: $pod ---"
              kubectl logs -n darwin "$pod" --tail=50 || true
            done
            echo ""
            echo "=== Helm test results (if any) ==="
            helm test darwin -n darwin || true
            exit 1
          fi
          
          # Verify deployment was successful
          echo "ðŸ” Verifying deployment..."
          if ! helm list -n darwin | grep -q darwin; then
            echo "âŒ Helm release 'darwin' not found after start.sh completed"
            exit 1
          fi
          
          echo "âœ… Deployment completed successfully"
      - name: Wait for darwin-workflow pod to be ready
        run: |
          echo "â³ Waiting for darwin-workflow pod to be ready..."
          export KUBECONFIG=./kind/config/kindkubeconfig.yaml
          
          max_attempts=60
          attempt=0
          while [ $attempt -lt $max_attempts ]; do
            # Check if pod exists and is ready
            if kubectl get pods -n darwin -l app.kubernetes.io/name=darwin-workflow 2>/dev/null | grep -q "Running\|1/1\|2/2"; then
              echo "âœ… darwin-workflow pod is ready"
              kubectl get pods -n darwin -l app.kubernetes.io/name=darwin-workflow
              break
            fi
            echo "Attempt $((attempt+1))/$max_attempts: Waiting for pod..."
            kubectl get pods -n darwin -l app.kubernetes.io/name=darwin-workflow || true
            sleep 5
            attempt=$((attempt+1))
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo "âŒ darwin-workflow pod did not become ready"
            echo "Pod status:"
            kubectl get pods -n darwin -l app.kubernetes.io/name=darwin-workflow || true
            echo "Pod logs:"
            kubectl logs -n darwin -l app.kubernetes.io/name=darwin-workflow --tail=50 || true
            exit 1
          fi
      - name: Verify Services Are Healthy (as per root README)
        run: |
          echo "âœ… Verifying services are healthy (as per root README)..."
          export KUBECONFIG=./kind/config/kindkubeconfig.yaml
          
          # Check all pods are running (as per README: kubectl get pods -n darwin)
          echo "ðŸ“¦ Checking all pods are running:"
          kubectl get pods -n darwin
          
          # Wait for ingress to be ready (if available)
          echo "â³ Waiting for ingress controller to be ready..."
          max_attempts=30
          attempt=0
          while [ $attempt -lt $max_attempts ]; do
            if kubectl get pods -n ingress-nginx 2>/dev/null | grep -q "Running"; then
              echo "âœ… Ingress controller is ready"
              break
            fi
            sleep 2
            attempt=$((attempt+1))
          done
          
          # Test health check via ingress (as per README: curl http://localhost/workflow/healthcheck)
          echo "ðŸ¥ Testing health check via ingress..."
          # Note: In CI, we may need to use port-forward instead if ingress isn't accessible
          # Try ingress first, fallback to port-forward
          
          # Start port-forward in background (as per README alternative: kubectl port-forward svc/darwin-workflow -n darwin 8001:8001)
          kubectl port-forward svc/darwin-workflow -n darwin 8001:8001 > /tmp/port-forward.log 2>&1 &
          PORT_FORWARD_PID=$!
          echo "Port-forward started with PID: $PORT_FORWARD_PID"
          
          # Wait for port-forward to be ready
          sleep 5
          
          # Test health check endpoint (as per README: curl http://localhost:8001/healthcheck)
          max_attempts=30
          attempt=0
          while [ $attempt -lt $max_attempts ]; do
            if curl -f -s http://localhost:8001/healthcheck > /dev/null 2>&1; then
              echo "âœ… Health check endpoint is accessible"
              break
            fi
            echo "Attempt $((attempt+1))/$max_attempts: Waiting for health check endpoint..."
            sleep 2
            attempt=$((attempt+1))
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo "âŒ Health check endpoint not accessible"
            echo "Port-forward logs:"
            cat /tmp/port-forward.log || true
            kill $PORT_FORWARD_PID 2>/dev/null || true
            exit 1
          fi
          
          # Test health check endpoint and verify response (as per README expected response)
          echo "Testing /healthcheck endpoint..."
          response=$(curl -s -w "\nHTTP_CODE:%{http_code}" http://localhost:8001/healthcheck)
          http_code=$(echo "$response" | grep "HTTP_CODE" | cut -d: -f2)
          body=$(echo "$response" | grep -v "HTTP_CODE")
          
          echo "HTTP Status Code: $http_code"
          echo "Response Body: $body"
          
          if [ "$http_code" != "200" ]; then
            echo "âŒ Health check returned HTTP $http_code, expected 200"
            kill $PORT_FORWARD_PID 2>/dev/null || true
            exit 1
          fi
          
          # Verify response structure matches README expected response
          python3 << EOF
          import json
          import sys
          
          try:
              data = json.loads('''$body''')
              required_fields = ['app_layer', 'core', 'db']
              missing_fields = [f for f in required_fields if f not in data]
              
              if missing_fields:
                  print(f"âŒ Missing required fields: {missing_fields}")
                  sys.exit(1)
              
              print("âœ… Health check response structure is valid")
              print(f"   app_layer: {data.get('app_layer')}")
              print(f"   core: {data.get('core')}")
              print(f"   db: {data.get('db')}")
              
              # Verify values match expected format from README
              if data.get('app_layer') != 'OK':
                  print(f"âš ï¸  app_layer is '{data.get('app_layer')}', expected 'OK'")
              
              if data.get('core') != 'OK':
                  print(f"âš ï¸  core is '{data.get('core')}', expected 'OK'")
              
              # db can be True/False (string or boolean)
              db_value = data.get('db')
              if db_value not in [True, False, 'True', 'False', 'true', 'false']:
                  print(f"âš ï¸  db value '{db_value}' is unexpected")
              
              print("\nâœ… Health check passed! Service is healthy (as per root README).")
          except json.JSONDecodeError as e:
              print(f"âŒ Invalid JSON response: {e}")
              sys.exit(1)
          EOF
          
          # Cleanup port-forward
          kill $PORT_FORWARD_PID 2>/dev/null || true
      - name: Cleanup
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up..."
          export KUBECONFIG=./kind/config/kindkubeconfig.yaml || true
          
          # Clean up Docker to free space (important for GitHub Actions runners)
          echo "Cleaning up Docker resources..."
          docker system prune -a -f || true
          docker volume prune -f || true
          
          # Kill any port-forwards
          pkill -f "kubectl port-forward" || true
          
          # Delete kind cluster if it exists
          if kind get clusters 2>/dev/null | grep -q kind; then
            echo "Deleting kind cluster..."
            kind delete cluster --name kind || true
          fi
          
          # Cleanup Docker resources
          docker system prune -f || true