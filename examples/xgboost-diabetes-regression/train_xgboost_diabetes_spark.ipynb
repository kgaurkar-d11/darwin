{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes Progression Prediction: Spark Data Processing + XGBoost Training\n",
        "\n",
        "This example demonstrates a hybrid approach using **Spark for data processing** and **native XGBoost for model training**.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "- **Data Processing**: PySpark for distributed ETL operations (scales to large datasets)\n",
        "- **Training**: Native XGBoost (efficient gradient boosting on driver)\n",
        "- **Model Logging**: MLflow xgboost flavor (reliable and compatible)\n",
        "\n",
        "## Execution Modes\n",
        "\n",
        "This notebook supports two execution modes:\n",
        "- **Darwin Cluster Mode**: Uses Darwin SDK with Ray for distributed Spark processing\n",
        "- **Local Mode**: Uses local Spark session for development/testing\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- **Name**: Diabetes Dataset\n",
        "- **Samples**: 442\n",
        "- **Features**: 10 baseline variables\n",
        "- **Target**: Quantitative measure of disease progression one year after baseline\n",
        "- **Type**: Regression\n",
        "\n",
        "## Model\n",
        "\n",
        "- **Framework**: Native XGBoost\n",
        "- **Data Processing**: PySpark\n",
        "- **Objective**: Squared error regression\n",
        "\n",
        "## Features\n",
        "\n",
        "The dataset includes 10 baseline variables:\n",
        "- `age`: Age in years\n",
        "- `sex`: Sex\n",
        "- `bmi`: Body mass index\n",
        "- `bp`: Average blood pressure\n",
        "- `s1`: Total serum cholesterol\n",
        "- `s2`: Low-density lipoproteins\n",
        "- `s3`: High-density lipoproteins\n",
        "- `s4`: Total cholesterol / HDL\n",
        "- `s5`: Log of serum triglycerides level\n",
        "- `s6`: Blood sugar level\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Spark handles data loading, transformation, and splitting (can scale to big data)\n",
        "- Native XGBoost handles model training (efficient and fast)\n",
        "- Model logged using `mlflow.xgboost` flavor (works with any MLflow server)\n",
        "- Fast model loading at serving time (no Spark dependencies needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix pyOpenSSL/cryptography compatibility issue first\n",
        "%pip install --upgrade pyOpenSSL cryptography\n",
        "\n",
        "# Install main dependencies (pin MLflow to match server version)\n",
        "%pip install xgboost pandas numpy scikit-learn mlflow==2.12.2 pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# XGBoost imports\n",
        "import xgboost as xgb\n",
        "\n",
        "# Spark imports (for data processing only)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# MLflow imports\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from mlflow import set_tracking_uri, set_experiment\n",
        "from mlflow.client import MlflowClient\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Scikit-learn imports (for loading dataset and metrics)\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Darwin SDK imports (optional - only available on Darwin cluster)\n",
        "DARWIN_SDK_AVAILABLE = False\n",
        "try:\n",
        "    import ray\n",
        "    from darwin import init_spark_with_configs, stop_spark\n",
        "    DARWIN_SDK_AVAILABLE = True\n",
        "    print(\"Darwin SDK available - will use distributed Spark on Darwin cluster\")\n",
        "except ImportError as e:\n",
        "    print(f\"Darwin SDK not available: {e}\")\n",
        "    print(\"Running in LOCAL mode - will use local Spark session\")\n",
        "except AttributeError as e:\n",
        "    # This typically happens with pyOpenSSL/cryptography version mismatch\n",
        "    if \"X509_V_FLAG\" in str(e) or \"lib\" in str(e):\n",
        "        print(\"=\" * 80)\n",
        "        print(\"ERROR: pyOpenSSL/cryptography version conflict detected!\")\n",
        "        print(\"Please run the following in a cell before importing:\")\n",
        "        print(\"  %pip install --upgrade pyOpenSSL cryptography\")\n",
        "        print(\"Then restart the kernel and try again.\")\n",
        "        print(\"=\" * 80)\n",
        "        raise\n",
        "    else:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_spark():\n",
        "    \"\"\"Initialize Spark session for data processing.\n",
        "    \n",
        "    Uses Darwin SDK on cluster, local Spark otherwise.\n",
        "    Spark is used for distributed data processing (ETL, splitting).\n",
        "    Training is done with native XGBoost on the driver.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"INITIALIZING SPARK SESSION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Base Spark configurations for data processing\n",
        "    spark_configs = {\n",
        "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
        "        \"spark.sql.session.timeZone\": \"UTC\",\n",
        "        \"spark.sql.shuffle.partitions\": \"4\",\n",
        "        \"spark.default.parallelism\": \"4\",\n",
        "        \"spark.executor.memory\": \"1g\",\n",
        "        \"spark.executor.cores\": \"1\",\n",
        "        \"spark.driver.memory\": \"1g\",\n",
        "        \"spark.executor.instances\": \"2\",\n",
        "    }\n",
        "    \n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        # Running on Darwin cluster - use distributed Spark via Ray\n",
        "        print(\"Mode: Darwin Cluster (Distributed)\")\n",
        "        ray.init()\n",
        "        spark = init_spark_with_configs(spark_configs=spark_configs)\n",
        "    else:\n",
        "        # Running locally - use local Spark session\n",
        "        print(\"Mode: Local Spark Session\")\n",
        "        builder = SparkSession.builder \\\n",
        "            .appName(\"Diabetes-Spark-DataProcessing\") \\\n",
        "            .master(\"local[*]\")\n",
        "        \n",
        "        for key, value in spark_configs.items():\n",
        "            builder = builder.config(key, value)\n",
        "        \n",
        "        spark = builder.getOrCreate()\n",
        "    \n",
        "    print(f\"Spark version: {spark.version}\")\n",
        "    print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "    \n",
        "    return spark\n",
        "\n",
        "\n",
        "def cleanup_spark(spark):\n",
        "    \"\"\"Stop Spark session properly based on environment.\"\"\"\n",
        "    print(\"\\nStopping Spark session...\")\n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        stop_spark()\n",
        "    else:\n",
        "        spark.stop()\n",
        "    print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_mlflow(mlflow_uri: str, username: str, password: str) -> MlflowClient:\n",
        "    \"\"\"Configure MLflow tracking and return client.\"\"\"\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = username\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = password\n",
        "    \n",
        "    set_tracking_uri(mlflow_uri)\n",
        "    client = MlflowClient(mlflow_uri)\n",
        "    \n",
        "    print(f\"MLflow tracking URI: {mlflow_uri}\")\n",
        "    return client\n",
        "\n",
        "\n",
        "def load_and_prepare_data(spark: SparkSession):\n",
        "    \"\"\"Load Diabetes dataset using Spark for processing, return pandas for training.\n",
        "    \n",
        "    Uses Spark for distributed data operations (can scale to large datasets).\n",
        "    Returns pandas DataFrames for XGBoost training.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load dataset\n",
        "    data = load_diabetes(as_frame=True)\n",
        "    pdf = data.data.copy()\n",
        "    pdf['target'] = data.target\n",
        "    \n",
        "    feature_names = data.feature_names\n",
        "    \n",
        "    print(f\"Dataset: Diabetes\")\n",
        "    print(f\"Samples: {len(pdf):,}\")\n",
        "    print(f\"Features: {len(feature_names)}\")\n",
        "    \n",
        "    print(f\"\\nFeature names:\")\n",
        "    for i, col in enumerate(feature_names, 1):\n",
        "        print(f\"  {i}. {col}\")\n",
        "    \n",
        "    print(f\"\\nTarget statistics:\")\n",
        "    print(f\"  Mean: {pdf['target'].mean():.2f}\")\n",
        "    print(f\"  Std: {pdf['target'].std():.2f}\")\n",
        "    print(f\"  Min: {pdf['target'].min():.2f}\")\n",
        "    print(f\"  Max: {pdf['target'].max():.2f}\")\n",
        "    \n",
        "    # Use Spark for distributed data splitting (demonstrates Spark processing)\n",
        "    print(\"\\nUsing Spark for distributed data splitting...\")\n",
        "    spark_df = spark.createDataFrame(pdf)\n",
        "    train_spark, test_spark = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
        "    \n",
        "    # Collect to pandas for XGBoost training\n",
        "    print(\"Collecting to pandas for training...\")\n",
        "    train_pdf = train_spark.toPandas()\n",
        "    test_pdf = test_spark.toPandas()\n",
        "    \n",
        "    print(f\"\\nTrain samples: {len(train_pdf):,}\")\n",
        "    print(f\"Test samples: {len(test_pdf):,}\")\n",
        "    \n",
        "    return train_pdf, test_pdf, feature_names\n",
        "\n",
        "\n",
        "def train_model(train_pdf, test_pdf, feature_names, hyperparams: dict):\n",
        "    \"\"\"Train XGBoost model using native XGBoost.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING MODEL (Native XGBoost)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in hyperparams.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    X_train = train_pdf[feature_names].values\n",
        "    y_train = train_pdf[\"target\"].values\n",
        "    X_test = test_pdf[feature_names].values\n",
        "    y_test = test_pdf[\"target\"].values\n",
        "    \n",
        "    print(f\"\\nTraining samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "    \n",
        "    # Create DMatrix for XGBoost\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=list(feature_names))\n",
        "    dtest = xgb.DMatrix(X_test, label=y_test, feature_names=list(feature_names))\n",
        "    \n",
        "    # XGBoost parameters\n",
        "    params = {\n",
        "        \"objective\": hyperparams.get(\"objective\", \"reg:squarederror\"),\n",
        "        \"max_depth\": hyperparams.get(\"max_depth\", 5),\n",
        "        \"learning_rate\": hyperparams.get(\"learning_rate\", 0.1),\n",
        "        \"subsample\": hyperparams.get(\"subsample\", 0.8),\n",
        "        \"colsample_bytree\": hyperparams.get(\"colsample_bytree\", 0.8),\n",
        "        \"seed\": hyperparams.get(\"random_state\", 42),\n",
        "    }\n",
        "    \n",
        "    # Train model\n",
        "    print(\"\\nTraining XGBoost model...\")\n",
        "    model = xgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=hyperparams.get(\"n_estimators\", 100),\n",
        "        evals=[(dtrain, \"train\"), (dtest, \"test\")],\n",
        "        verbose_eval=False\n",
        "    )\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(dtrain)\n",
        "    y_test_pred = model.predict(dtest)\n",
        "    \n",
        "    return model, X_train, y_train, X_test, y_test, y_train_pred, y_test_pred\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, dataset_name=\"Test\"):\n",
        "    \"\"\"Calculate evaluation metrics using sklearn.\"\"\"\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    return {\n",
        "        f\"{dataset_name.lower()}_rmse\": rmse,\n",
        "        f\"{dataset_name.lower()}_mae\": mae,\n",
        "        f\"{dataset_name.lower()}_r2\": r2\n",
        "    }\n",
        "\n",
        "\n",
        "def log_to_mlflow(model, X_train, y_train, y_test, y_train_pred, y_test_pred, \n",
        "                  hyperparams, feature_names):\n",
        "    \"\"\"Log XGBoost model, parameters, and metrics to MLflow.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOGGING TO MLFLOW\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Log hyperparameters\n",
        "    for key, value in hyperparams.items():\n",
        "        mlflow.log_param(key, value)\n",
        "    \n",
        "    # Log additional params\n",
        "    mlflow.log_param(\"training_framework\", \"xgboost\")\n",
        "    mlflow.log_param(\"data_processing\", \"spark\")\n",
        "    \n",
        "    # Calculate and log metrics\n",
        "    train_metrics = calculate_metrics(y_train, y_train_pred, dataset_name=\"Train\")\n",
        "    test_metrics = calculate_metrics(y_test, y_test_pred, dataset_name=\"Test\")\n",
        "    all_metrics = {**train_metrics, **test_metrics}\n",
        "    \n",
        "    for metric_name, metric_value in all_metrics.items():\n",
        "        mlflow.log_metric(metric_name, metric_value)\n",
        "    \n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"  Training RMSE: {train_metrics['train_rmse']:.4f}\")\n",
        "    print(f\"  Training R2: {train_metrics['train_r2']:.4f}\")\n",
        "    print(f\"  Test RMSE: {test_metrics['test_rmse']:.4f}\")\n",
        "    print(f\"  Test MAE: {test_metrics['test_mae']:.4f}\")\n",
        "    print(f\"  Test R2: {test_metrics['test_r2']:.4f}\")\n",
        "    \n",
        "    # Log model using mlflow.xgboost\n",
        "    model_logged = False\n",
        "    print(\"\\nSaving model artifacts...\")\n",
        "    try:\n",
        "        # Create sample input for signature\n",
        "        sample_input = pd.DataFrame([X_train[0]], columns=feature_names)\n",
        "        sample_output = pd.DataFrame({\"prediction\": [0.0]})\n",
        "        signature = infer_signature(sample_input, sample_output)\n",
        "        \n",
        "        # Log XGBoost model\n",
        "        print(\"  Logging to MLflow using xgboost flavor...\")\n",
        "        mlflow.xgboost.log_model(\n",
        "            xgb_model=model,\n",
        "            artifact_path=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=sample_input\n",
        "        )\n",
        "        \n",
        "        model_logged = True\n",
        "        print(\"Model artifacts logged successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log model artifacts: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Metrics and parameters were logged successfully.\")\n",
        "    \n",
        "    # Store native model reference for later use\n",
        "    all_metrics[\"_native_model\"] = model\n",
        "    all_metrics[\"_model_logged\"] = model_logged\n",
        "    \n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "def register_model(client: MlflowClient, model_name: str, run_id: str, experiment_id: str):\n",
        "    \"\"\"Register model in MLflow Model Registry.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"REGISTERING MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "    \n",
        "    # Create registered model if it doesn't exist\n",
        "    try:\n",
        "        client.get_registered_model(model_name)\n",
        "        print(f\"Model '{model_name}' already exists in registry\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            client.create_registered_model(model_name)\n",
        "            print(f\"Created registered model: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create registered model: {e}\")\n",
        "    \n",
        "    # Create model version\n",
        "    try:\n",
        "        result = client.create_model_version(\n",
        "            name=model_name,\n",
        "            source=model_uri,\n",
        "            run_id=run_id\n",
        "        )\n",
        "        print(f\"Model version registered successfully!\")\n",
        "        print(f\"   Model Name: {model_name}\")\n",
        "        print(f\"   Version: {result.version}\")\n",
        "        print(f\"   Run ID: {run_id}\")\n",
        "        return result.version\n",
        "    except Exception as e:\n",
        "        print(f\"Model registration failed (model still usable via run URI): {e}\")\n",
        "        print(f\"   You can deploy using: mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_model_and_predict(model_uri: str, sample_data: pd.DataFrame, native_model=None):\n",
        "    \"\"\"Load model from MLflow URI and run prediction.\n",
        "    \n",
        "    Args:\n",
        "        model_uri: MLflow model URI (e.g., runs:/{run_id}/model)\n",
        "        sample_data: Pandas DataFrame with feature data\n",
        "        native_model: Optional - use this model directly instead of loading from MLflow\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING MODEL AND RUNNING PREDICTION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if native_model is not None:\n",
        "        # Use the native model directly (no need to load from MLflow)\n",
        "        print(\"Using in-memory model for prediction\")\n",
        "        loaded_model = native_model\n",
        "    else:\n",
        "        # Try to load from MLflow\n",
        "        print(f\"Model URI: {model_uri}\")\n",
        "        try:\n",
        "            # Try MLflow's xgboost loader first\n",
        "            loaded_model = mlflow.xgboost.load_model(model_uri)\n",
        "            print(\"Model loaded from MLflow successfully!\")\n",
        "        except Exception as e:\n",
        "            # Fallback: Download artifact and load manually\n",
        "            print(f\"MLflow loader failed, trying artifact download: {e}\")\n",
        "            try:\n",
        "                client = MlflowClient()\n",
        "                run_id = model_uri.split(\"/\")[1] if model_uri.startswith(\"runs:/\") else model_uri\n",
        "                \n",
        "                with tempfile.TemporaryDirectory() as tmpdir:\n",
        "                    local_path = client.download_artifacts(run_id, \"model/model.json\", tmpdir)\n",
        "                    loaded_model = xgb.Booster()\n",
        "                    loaded_model.load_model(local_path)\n",
        "                    print(\"Model loaded from artifact successfully!\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Could not load model: {e2}\")\n",
        "                raise\n",
        "    \n",
        "    # Create DMatrix for prediction\n",
        "    dmatrix = xgb.DMatrix(sample_data)\n",
        "    \n",
        "    # Run prediction\n",
        "    predictions = loaded_model.predict(dmatrix)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAMPLE PREDICTION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nInput features:\")\n",
        "    for col in sample_data.columns:\n",
        "        print(f\"  {col}: {sample_data[col].iloc[0]:.4f}\")\n",
        "    print(f\"\\nPredicted diabetes progression: {predictions[0]:.2f}\")\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "\n",
        "def print_deployment_info(run_id: str, experiment_id: str, model_name: str, model_version: str):\n",
        "    \"\"\"Print deployment instructions and sample payloads.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nRun Information:\")\n",
        "    print(f\"  Run ID: {run_id}\")\n",
        "    print(f\"  Experiment ID: {experiment_id}\")\n",
        "    print(f\"  Model URI (run): runs:/{run_id}/model\")\n",
        "    if model_version:\n",
        "        print(f\"  Model URI (registry): models:/{model_name}/{model_version}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DEPLOYMENT PAYLOAD (deploy-model API)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    deploy_payload = {\n",
        "        \"serve_name\": \"diabetes-xgboost-spark-regressor\",\n",
        "        \"model_uri\": f\"mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\",\n",
        "        \"env\": \"local\",\n",
        "        \"cores\": 2,\n",
        "        \"memory\": 4,\n",
        "        \"node_capacity\": \"spot\",\n",
        "        \"min_replicas\": 1,\n",
        "        \"max_replicas\": 3\n",
        "    }\n",
        "    \n",
        "    print(json.dumps(deploy_payload, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train Diabetes Regression Model (Spark data processing + XGBoost training)\")\n",
        "    parser.add_argument(\n",
        "        \"--mlflow-uri\",\n",
        "        default=\"http://darwin-mlflow-lib.darwin.svc.cluster.local:8080\",\n",
        "        help=\"MLflow tracking URI\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--username\",\n",
        "        default=\"abc@gmail.com\",\n",
        "        help=\"MLflow username\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--password\",\n",
        "        default=\"password\",\n",
        "        help=\"MLflow password\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--experiment-name\",\n",
        "        default=\"diabetes_spark_xgboost_regression\",\n",
        "        help=\"MLflow experiment name\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model-name\",\n",
        "        default=\"DiabetesXGBoostRegressor\",\n",
        "        help=\"Registered model name\"\n",
        "    )\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DIABETES REGRESSION: SPARK DATA PROCESSING + XGBOOST TRAINING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Initialize Spark for data processing\n",
        "    spark = initialize_spark()\n",
        "    \n",
        "    # Setup MLflow\n",
        "    client = setup_mlflow(args.mlflow_uri, args.username, args.password)\n",
        "    set_experiment(experiment_name=args.experiment_name)\n",
        "    print(f\"Experiment: {args.experiment_name}\")\n",
        "    \n",
        "    # Load and prepare data using Spark (returns pandas DataFrames)\n",
        "    train_pdf, test_pdf, feature_names = load_and_prepare_data(spark)\n",
        "    \n",
        "    # Define hyperparameters\n",
        "    hyperparams = {\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"max_depth\": 5,\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"n_estimators\": 100,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.8,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    \n",
        "    # Start MLflow run\n",
        "    with mlflow.start_run(run_name=f\"xgboost_diabetes_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "        # Train XGBoost model\n",
        "        model, X_train, y_train, X_test, y_test, y_train_pred, y_test_pred = train_model(\n",
        "            train_pdf, test_pdf, feature_names, hyperparams\n",
        "        )\n",
        "        \n",
        "        # Log to MLflow\n",
        "        metrics = log_to_mlflow(\n",
        "            model, X_train, y_train, y_test, y_train_pred, y_test_pred,\n",
        "            hyperparams, feature_names\n",
        "        )\n",
        "        \n",
        "        # Get run information\n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "        experiment_id = mlflow.active_run().info.experiment_id\n",
        "    \n",
        "    # Register model (outside of run context) - only if artifacts were logged\n",
        "    model_version = None\n",
        "    if metrics.get(\"_model_logged\", False):\n",
        "        model_version = register_model(client, args.model_name, run_id, experiment_id)\n",
        "    else:\n",
        "        print(\"\\nSkipping model registration (artifacts not logged to MLflow)\")\n",
        "    \n",
        "    # Demo prediction with XGBoost model\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAMPLE PREDICTION\")\n",
        "    print(\"=\" * 80)\n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "    sample_pdf = test_pdf[feature_names].head(1)\n",
        "    \n",
        "    # Use the native model for prediction\n",
        "    native_model = metrics.get(\"_native_model\")\n",
        "    predictions = load_model_and_predict(model_uri, sample_pdf, native_model=native_model)\n",
        "    \n",
        "    # Get actual value for comparison\n",
        "    actual_value = test_pdf[\"target\"].iloc[0]\n",
        "    print(f\"\\nActual diabetes progression: {actual_value:.2f}\")\n",
        "    print(f\"Prediction error: {abs(predictions[0] - actual_value):.2f}\")\n",
        "    \n",
        "    # Print deployment information\n",
        "    print_deployment_info(run_id, experiment_id, args.model_name, model_version)\n",
        "    \n",
        "    # Cleanup: Stop Spark session\n",
        "    cleanup_spark(spark)\n",
        "    \n",
        "    print(\"\\nScript completed successfully!\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
