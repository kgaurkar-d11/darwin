{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iris Species Classification: Spark Data Processing + Sklearn Training\n",
        "\n",
        "This example demonstrates a hybrid approach using **Spark for data processing** and **sklearn for model training**.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "- **Data Processing**: PySpark for distributed ETL operations (scales to large datasets)\n",
        "- **Training**: sklearn RandomForestClassifier (reliable, fast, easy to deploy)\n",
        "- **Model Logging**: MLflow sklearn flavor (compatible with all MLflow servers)\n",
        "\n",
        "## Execution Modes\n",
        "\n",
        "This notebook supports two execution modes:\n",
        "- **Darwin Cluster Mode**: Uses Darwin SDK with Ray for distributed Spark processing\n",
        "- **Local Mode**: Uses local Spark session for development/testing\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- **Name**: Iris Dataset (Fisher, 1936)\n",
        "- **Samples**: 150 iris flowers\n",
        "- **Target**: Species (Setosa, Versicolor, Virginica)\n",
        "- **Type**: Multi-class Classification\n",
        "\n",
        "## Model\n",
        "\n",
        "- **Framework**: sklearn RandomForestClassifier\n",
        "- **Data Processing**: PySpark\n",
        "- **Objective**: Multi-class classification\n",
        "\n",
        "## Features\n",
        "\n",
        "The dataset includes 4 measurements:\n",
        "- `sepal_length`: Sepal length (cm)\n",
        "- `sepal_width`: Sepal width (cm)\n",
        "- `petal_length`: Petal length (cm)\n",
        "- `petal_width`: Petal width (cm)\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Spark handles data loading, transformation, and splitting (can scale to big data)\n",
        "- sklearn handles model training (reliable MLflow integration)\n",
        "- Model logged using `mlflow.sklearn` flavor (works with any MLflow server)\n",
        "- Fast model loading at serving time (no Java/Spark dependencies needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix pyOpenSSL/cryptography compatibility issue first\n",
        "%pip install --upgrade pyOpenSSL cryptography\n",
        "\n",
        "# Install main dependencies (pin MLflow to match server version)\n",
        "%pip install pandas numpy scikit-learn mlflow==2.12.2 pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Spark imports (for data processing only)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# MLflow imports\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow import set_tracking_uri, set_experiment\n",
        "from mlflow.client import MlflowClient\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Scikit-learn imports (for training and metrics)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Darwin SDK imports (optional - only available on Darwin cluster)\n",
        "DARWIN_SDK_AVAILABLE = False\n",
        "try:\n",
        "    import ray\n",
        "    from darwin import init_spark_with_configs, stop_spark\n",
        "    DARWIN_SDK_AVAILABLE = True\n",
        "    print(\"Darwin SDK available - will use distributed Spark on Darwin cluster\")\n",
        "except ImportError as e:\n",
        "    print(f\"Darwin SDK not available: {e}\")\n",
        "    print(\"Running in LOCAL mode - will use local Spark session\")\n",
        "except AttributeError as e:\n",
        "    if \"X509_V_FLAG\" in str(e) or \"lib\" in str(e):\n",
        "        print(\"=\" * 80)\n",
        "        print(\"ERROR: pyOpenSSL/cryptography version conflict detected!\")\n",
        "        print(\"Please run the following in a cell before importing:\")\n",
        "        print(\"  %pip install --upgrade pyOpenSSL cryptography\")\n",
        "        print(\"Then restart the kernel and try again.\")\n",
        "        print(\"=\" * 80)\n",
        "        raise\n",
        "    else:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_spark():\n",
        "    \"\"\"Initialize Spark session for data processing.\n",
        "    \n",
        "    Uses Darwin SDK on cluster, local Spark otherwise.\n",
        "    Spark is used for distributed data processing (ETL, splitting).\n",
        "    Training is done with sklearn on the driver.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"INITIALIZING SPARK SESSION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Base Spark configurations for data processing\n",
        "    spark_configs = {\n",
        "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
        "        \"spark.sql.session.timeZone\": \"UTC\",\n",
        "        \"spark.sql.shuffle.partitions\": \"4\",\n",
        "        \"spark.default.parallelism\": \"4\",\n",
        "        \"spark.executor.memory\": \"1g\",\n",
        "        \"spark.executor.cores\": \"1\",\n",
        "        \"spark.driver.memory\": \"1g\",\n",
        "        \"spark.executor.instances\": \"2\",\n",
        "    }\n",
        "    \n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        # Running on Darwin cluster - use distributed Spark via Ray\n",
        "        print(\"Mode: Darwin Cluster (Distributed)\")\n",
        "        ray.init()\n",
        "        spark = init_spark_with_configs(spark_configs=spark_configs)\n",
        "    else:\n",
        "        # Running locally - use local Spark session\n",
        "        print(\"Mode: Local Spark Session\")\n",
        "        builder = SparkSession.builder \\\n",
        "            .appName(\"Iris-Spark-DataProcessing\") \\\n",
        "            .master(\"local[*]\")\n",
        "        \n",
        "        for key, value in spark_configs.items():\n",
        "            builder = builder.config(key, value)\n",
        "        \n",
        "        spark = builder.getOrCreate()\n",
        "    \n",
        "    print(f\"Spark version: {spark.version}\")\n",
        "    print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "    \n",
        "    return spark\n",
        "\n",
        "\n",
        "def cleanup_spark(spark):\n",
        "    \"\"\"Stop Spark session properly based on environment.\"\"\"\n",
        "    print(\"\\nStopping Spark session...\")\n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        stop_spark()\n",
        "    else:\n",
        "        spark.stop()\n",
        "    print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_mlflow(mlflow_uri: str, username: str, password: str) -> MlflowClient:\n",
        "    \"\"\"Configure MLflow tracking and return client.\"\"\"\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = username\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = password\n",
        "    \n",
        "    set_tracking_uri(mlflow_uri)\n",
        "    client = MlflowClient(mlflow_uri)\n",
        "    \n",
        "    print(f\"MLflow tracking URI: {mlflow_uri}\")\n",
        "    return client\n",
        "\n",
        "\n",
        "def load_and_prepare_data(spark: SparkSession):\n",
        "    \"\"\"Load Iris dataset using Spark for processing, return pandas for training.\n",
        "    \n",
        "    Uses Spark for distributed data operations (can scale to large datasets).\n",
        "    Returns pandas DataFrames for sklearn training.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load dataset\n",
        "    iris = load_iris(as_frame=True)\n",
        "    pdf = iris.data.copy()\n",
        "    pdf.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "    pdf['label'] = iris.target\n",
        "    \n",
        "    target_names = iris.target_names.tolist()\n",
        "    feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "    \n",
        "    print(f\"Dataset: Iris\")\n",
        "    print(f\"Samples: {len(pdf):,}\")\n",
        "    print(f\"Features: {len(feature_names)}\")\n",
        "    \n",
        "    print(f\"\\nFeature names:\")\n",
        "    for i, name in enumerate(feature_names, 1):\n",
        "        print(f\"  {i}. {name}\")\n",
        "    \n",
        "    print(f\"\\nTarget classes:\")\n",
        "    for i, name in enumerate(target_names):\n",
        "        count = (pdf['label'] == i).sum()\n",
        "        print(f\"  {i}. {name}: {count} samples\")\n",
        "    \n",
        "    # Use Spark for distributed data splitting (demonstrates Spark processing)\n",
        "    print(\"\\nUsing Spark for distributed data splitting...\")\n",
        "    spark_df = spark.createDataFrame(pdf)\n",
        "    train_spark, test_spark = spark_df.randomSplit([0.8, 0.2], seed=42)\n",
        "    \n",
        "    # Collect to pandas for sklearn training\n",
        "    print(\"Collecting to pandas for training...\")\n",
        "    train_pdf = train_spark.toPandas()\n",
        "    test_pdf = test_spark.toPandas()\n",
        "    \n",
        "    print(f\"\\nTrain samples: {len(train_pdf):,}\")\n",
        "    print(f\"Test samples: {len(test_pdf):,}\")\n",
        "    \n",
        "    return train_pdf, test_pdf, feature_names, target_names\n",
        "\n",
        "\n",
        "def train_model(train_pdf, test_pdf, feature_names, hyperparams: dict):\n",
        "    \"\"\"Train Random Forest model using sklearn.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING MODEL (sklearn RandomForest)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in hyperparams.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Prepare data\n",
        "    X_train = train_pdf[feature_names].values\n",
        "    y_train = train_pdf['label'].values\n",
        "    X_test = test_pdf[feature_names].values\n",
        "    y_test = test_pdf['label'].values\n",
        "    \n",
        "    # Create and train sklearn Random Forest\n",
        "    print(\"\\nTraining Random Forest model...\")\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=hyperparams.get(\"n_estimators\", 100),\n",
        "        max_depth=hyperparams.get(\"max_depth\", 10),\n",
        "        min_samples_leaf=hyperparams.get(\"min_samples_leaf\", 1),\n",
        "        random_state=hyperparams.get(\"random_state\", 42),\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Number of trees: {model.n_estimators}\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    return model, X_train, y_train, X_test, y_test, y_train_pred, y_test_pred\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, dataset_name=\"Test\"):\n",
        "    \"\"\"Calculate evaluation metrics using sklearn.\"\"\"\n",
        "    return {\n",
        "        f\"{dataset_name.lower()}_accuracy\": accuracy_score(y_true, y_pred),\n",
        "        f\"{dataset_name.lower()}_precision\": precision_score(y_true, y_pred, average='weighted'),\n",
        "        f\"{dataset_name.lower()}_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
        "        f\"{dataset_name.lower()}_f1\": f1_score(y_true, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "\n",
        "def log_to_mlflow(model, X_train, y_train, y_test, y_train_pred, y_test_pred, \n",
        "                  hyperparams, feature_names, target_names):\n",
        "    \"\"\"Log sklearn model, parameters, and metrics to MLflow.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOGGING TO MLFLOW\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Log hyperparameters\n",
        "    for key, value in hyperparams.items():\n",
        "        mlflow.log_param(key, value)\n",
        "    \n",
        "    # Log additional params\n",
        "    mlflow.log_param(\"training_framework\", \"sklearn\")\n",
        "    mlflow.log_param(\"data_processing\", \"spark\")\n",
        "    mlflow.log_param(\"num_trees\", model.n_estimators)\n",
        "    \n",
        "    # Calculate and log metrics\n",
        "    train_metrics = calculate_metrics(y_train, y_train_pred, dataset_name=\"Train\")\n",
        "    test_metrics = calculate_metrics(y_test, y_test_pred, dataset_name=\"Test\")\n",
        "    all_metrics = {**train_metrics, **test_metrics}\n",
        "    \n",
        "    for metric_name, metric_value in all_metrics.items():\n",
        "        mlflow.log_metric(metric_name, metric_value)\n",
        "    \n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"  Training Accuracy: {train_metrics['train_accuracy']:.4f}\")\n",
        "    print(f\"  Training F1: {train_metrics['train_f1']:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
        "    print(f\"  Test Precision: {test_metrics['test_precision']:.4f}\")\n",
        "    print(f\"  Test Recall: {test_metrics['test_recall']:.4f}\")\n",
        "    print(f\"  Test F1: {test_metrics['test_f1']:.4f}\")\n",
        "    \n",
        "    # Print confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    print(f\"\\n  Confusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "    \n",
        "    # Log model using mlflow.sklearn\n",
        "    model_logged = False\n",
        "    print(\"\\nSaving model artifacts...\")\n",
        "    try:\n",
        "        # Create sample input for signature\n",
        "        sample_input = pd.DataFrame([X_train[0]], columns=feature_names)\n",
        "        sample_output = pd.DataFrame({\"prediction\": [0]})\n",
        "        signature = infer_signature(sample_input, sample_output)\n",
        "        \n",
        "        # Log sklearn model\n",
        "        print(\"  Logging to MLflow using sklearn flavor...\")\n",
        "        mlflow.sklearn.log_model(\n",
        "            sk_model=model,\n",
        "            artifact_path=\"model\",\n",
        "            signature=signature,\n",
        "            input_example=sample_input\n",
        "        )\n",
        "        \n",
        "        # Also log target names as additional artifact\n",
        "        local_model_dir = tempfile.mkdtemp(prefix=\"sklearn_model_\")\n",
        "        target_names_file = os.path.join(local_model_dir, \"target_names.json\")\n",
        "        with open(target_names_file, \"w\") as f:\n",
        "            json.dump(target_names, f)\n",
        "        mlflow.log_artifact(target_names_file, artifact_path=\"model\")\n",
        "        \n",
        "        model_logged = True\n",
        "        print(\"Model artifacts logged successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log model artifacts: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(\"Metrics and parameters were logged successfully.\")\n",
        "    \n",
        "    all_metrics[\"_model_logged\"] = model_logged\n",
        "    \n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "def register_model(client: MlflowClient, model_name: str, run_id: str, experiment_id: str):\n",
        "    \"\"\"Register model in MLflow Model Registry.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"REGISTERING MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "    \n",
        "    # Create registered model if it doesn't exist\n",
        "    try:\n",
        "        client.get_registered_model(model_name)\n",
        "        print(f\"Model '{model_name}' already exists in registry\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            client.create_registered_model(model_name)\n",
        "            print(f\"Created registered model: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create registered model: {e}\")\n",
        "    \n",
        "    # Create model version\n",
        "    try:\n",
        "        result = client.create_model_version(\n",
        "            name=model_name,\n",
        "            source=model_uri,\n",
        "            run_id=run_id\n",
        "        )\n",
        "        print(f\"Model version registered successfully!\")\n",
        "        print(f\"   Model Name: {model_name}\")\n",
        "        print(f\"   Version: {result.version}\")\n",
        "        print(f\"   Run ID: {run_id}\")\n",
        "        return result.version\n",
        "    except Exception as e:\n",
        "        print(f\"Model registration failed (model still usable via run URI): {e}\")\n",
        "        print(f\"   You can deploy using: mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def print_deployment_info(run_id: str, experiment_id: str, model_name: str, model_version: str):\n",
        "    \"\"\"Print deployment instructions and sample payloads.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nRun Information:\")\n",
        "    print(f\"  Run ID: {run_id}\")\n",
        "    print(f\"  Experiment ID: {experiment_id}\")\n",
        "    print(f\"  Model URI (run): runs:/{run_id}/model\")\n",
        "    if model_version:\n",
        "        print(f\"  Model URI (registry): models:/{model_name}/{model_version}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DEPLOYMENT PAYLOAD (deploy-model API)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    deploy_payload = {\n",
        "        \"serve_name\": \"iris-rf-spark-classifier\",\n",
        "        \"model_uri\": f\"mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\",\n",
        "        \"env\": \"local\",\n",
        "        \"cores\": 2,\n",
        "        \"memory\": 4,\n",
        "        \"node_capacity\": \"spot\",\n",
        "        \"min_replicas\": 1,\n",
        "        \"max_replicas\": 3\n",
        "    }\n",
        "    \n",
        "    print(json.dumps(deploy_payload, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train Iris Classification Model (Spark data processing + sklearn training)\")\n",
        "    parser.add_argument(\n",
        "        \"--mlflow-uri\",\n",
        "        default=\"http://darwin-mlflow-lib.darwin.svc.cluster.local:8080\",\n",
        "        help=\"MLflow tracking URI\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--username\",\n",
        "        default=\"abc@gmail.com\",\n",
        "        help=\"MLflow username\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--password\",\n",
        "        default=\"password\",\n",
        "        help=\"MLflow password\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--experiment-name\",\n",
        "        default=\"iris_spark_sklearn_classification\",\n",
        "        help=\"MLflow experiment name\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model-name\",\n",
        "        default=\"IrisSklearnRFClassifier\",\n",
        "        help=\"Registered model name\"\n",
        "    )\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"IRIS CLASSIFICATION: SPARK DATA PROCESSING + SKLEARN TRAINING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Initialize Spark for data processing\n",
        "    spark = initialize_spark()\n",
        "    \n",
        "    # Setup MLflow\n",
        "    client = setup_mlflow(args.mlflow_uri, args.username, args.password)\n",
        "    set_experiment(experiment_name=args.experiment_name)\n",
        "    print(f\"Experiment: {args.experiment_name}\")\n",
        "    \n",
        "    # Load and prepare data using Spark (returns pandas DataFrames)\n",
        "    train_pdf, test_pdf, feature_names, target_names = load_and_prepare_data(spark)\n",
        "    \n",
        "    # Define hyperparameters\n",
        "    hyperparams = {\n",
        "        \"n_estimators\": 100,\n",
        "        \"max_depth\": 10,\n",
        "        \"min_samples_leaf\": 1,\n",
        "        \"random_state\": 42,\n",
        "    }\n",
        "    \n",
        "    # Start MLflow run\n",
        "    with mlflow.start_run(run_name=f\"sklearn_rf_iris_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "        # Train sklearn model\n",
        "        model, X_train, y_train, X_test, y_test, y_train_pred, y_test_pred = train_model(\n",
        "            train_pdf, test_pdf, feature_names, hyperparams\n",
        "        )\n",
        "        \n",
        "        # Log to MLflow\n",
        "        metrics = log_to_mlflow(\n",
        "            model, X_train, y_train, y_test, y_train_pred, y_test_pred,\n",
        "            hyperparams, feature_names, target_names\n",
        "        )\n",
        "        \n",
        "        # Get run information\n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "        experiment_id = mlflow.active_run().info.experiment_id\n",
        "    \n",
        "    # Register model (outside of run context) - only if artifacts were logged\n",
        "    model_version = None\n",
        "    if metrics.get(\"_model_logged\", False):\n",
        "        model_version = register_model(client, args.model_name, run_id, experiment_id)\n",
        "    else:\n",
        "        print(\"\\nSkipping model registration (artifacts not logged to MLflow)\")\n",
        "    \n",
        "    # Demo prediction with sklearn model\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAMPLE PREDICTION\")\n",
        "    print(\"=\" * 80)\n",
        "    sample_idx = 0\n",
        "    sample_features = X_test[sample_idx:sample_idx+1]\n",
        "    prediction = model.predict(sample_features)[0]\n",
        "    probabilities = model.predict_proba(sample_features)[0]\n",
        "    \n",
        "    print(f\"\\nInput features:\")\n",
        "    for i, name in enumerate(feature_names):\n",
        "        print(f\"  {name}: {sample_features[0][i]:.4f}\")\n",
        "    \n",
        "    print(f\"\\nClass Probabilities:\")\n",
        "    for i, prob in enumerate(probabilities):\n",
        "        print(f\"  {target_names[i]}: {prob:.4f}\")\n",
        "    \n",
        "    predicted_species = target_names[prediction]\n",
        "    actual_species = target_names[y_test[sample_idx]]\n",
        "    print(f\"\\nPredicted: {prediction} ({predicted_species})\")\n",
        "    print(f\"Actual: {y_test[sample_idx]} ({actual_species})\")\n",
        "    print(f\"Correct: {prediction == y_test[sample_idx]}\")\n",
        "    \n",
        "    # Print deployment information\n",
        "    print_deployment_info(run_id, experiment_id, args.model_name, model_version)\n",
        "    \n",
        "    # Cleanup: Stop Spark session\n",
        "    cleanup_spark(spark)\n",
        "    \n",
        "    print(\"\\nScript completed successfully!\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
