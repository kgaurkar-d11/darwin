{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iris Species Classification with Random Forest on Spark\n",
        "\n",
        "This example demonstrates **distributed training** of a Random Forest classifier using PySpark ML to predict iris species.\n",
        "\n",
        "## Execution Modes\n",
        "\n",
        "This notebook supports two execution modes:\n",
        "- **Darwin Cluster Mode**: Uses Darwin SDK with Ray for distributed Spark processing (automatic when running on Darwin cluster)\n",
        "- **Local Mode**: Uses local Spark session for development/testing (automatic fallback when Darwin SDK is not available)\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- **Name**: Iris Dataset (Fisher, 1936)\n",
        "- **Samples**: 150 iris flowers\n",
        "- **Target**: Species (Setosa, Versicolor, Virginica)\n",
        "- **Type**: Multi-class Classification\n",
        "\n",
        "## Model\n",
        "\n",
        "- **Framework**: PySpark ML RandomForestClassifier\n",
        "- **Type**: Distributed Random Forest Classifier\n",
        "- **Objective**: Multi-class classification\n",
        "\n",
        "## Features\n",
        "\n",
        "The dataset includes 4 measurements:\n",
        "- `sepal_length`: Sepal length (cm)\n",
        "- `sepal_width`: Sepal width (cm)\n",
        "- `petal_length`: Petal length (cm)\n",
        "- `petal_width`: Petal width (cm)\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Training uses PySpark ML `RandomForestClassifier` for distributed training\n",
        "- Data is processed as Spark DataFrames with `VectorAssembler`\n",
        "- Model is registered to MLflow Model Registry for versioning\n",
        "- Demonstrates loading model and running inference on driver\n",
        "- **Auto-installs required packages on Spark executors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix pyOpenSSL/cryptography compatibility issue first\n",
        "%pip install --upgrade pyOpenSSL cryptography\n",
        "\n",
        "# Install main dependencies\n",
        "%pip install pandas numpy scikit-learn mlflow pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Spark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier as SparkRFClassifier\n",
        "from pyspark.ml.classification import RandomForestClassificationModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# MLflow imports\n",
        "import mlflow\n",
        "import mlflow.spark\n",
        "from mlflow import set_tracking_uri, set_experiment\n",
        "from mlflow.client import MlflowClient\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Scikit-learn imports (for loading dataset and metrics)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Darwin SDK imports (optional - only available on Darwin cluster)\n",
        "DARWIN_SDK_AVAILABLE = False\n",
        "try:\n",
        "    import ray\n",
        "    from darwin import init_spark_with_configs, stop_spark\n",
        "    DARWIN_SDK_AVAILABLE = True\n",
        "    print(\"Darwin SDK available - will use distributed Spark on Darwin cluster\")\n",
        "except ImportError as e:\n",
        "    print(f\"Darwin SDK not available: {e}\")\n",
        "    print(\"Running in LOCAL mode - will use local Spark session\")\n",
        "except AttributeError as e:\n",
        "    if \"X509_V_FLAG\" in str(e) or \"lib\" in str(e):\n",
        "        print(\"=\" * 80)\n",
        "        print(\"ERROR: pyOpenSSL/cryptography version conflict detected!\")\n",
        "        print(\"Please run the following in a cell before importing:\")\n",
        "        print(\"  %pip install --upgrade pyOpenSSL cryptography\")\n",
        "        print(\"Then restart the kernel and try again.\")\n",
        "        print(\"=\" * 80)\n",
        "        raise\n",
        "    else:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def install_packages_on_executors(spark):\n",
        "    \"\"\"Install required packages on Spark executors using pip.\"\"\"\n",
        "    print(\"Installing packages on Spark executors...\")\n",
        "    \n",
        "    # Packages required for data processing on executors\n",
        "    EXECUTOR_PACKAGES = [\n",
        "        \"scikit-learn\",\n",
        "        \"pandas\",\n",
        "        \"numpy\",\n",
        "    ]\n",
        "    \n",
        "    def install_packages(iterator):\n",
        "        import subprocess\n",
        "        import sys\n",
        "        # Install all required packages on each executor\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \n",
        "            *EXECUTOR_PACKAGES,\n",
        "            \"-q\", \"--disable-pip-version-check\"\n",
        "        ])\n",
        "        yield True\n",
        "    \n",
        "    # Run installation on all executors\n",
        "    num_executors = spark.sparkContext.defaultParallelism\n",
        "    spark.sparkContext.parallelize(range(num_executors), num_executors) \\\n",
        "        .mapPartitions(install_packages) \\\n",
        "        .collect()\n",
        "    \n",
        "    print(f\"Packages installed on {num_executors} executors: {', '.join(EXECUTOR_PACKAGES)}\")\n",
        "\n",
        "\n",
        "def initialize_spark():\n",
        "    \"\"\"Initialize Spark session - uses Darwin SDK on cluster, local Spark otherwise.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"INITIALIZING SPARK SESSION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Base Spark configurations\n",
        "    # Resource settings optimized for smaller cluster nodes to avoid OOM\n",
        "    spark_configs = {\n",
        "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
        "        \"spark.sql.session.timeZone\": \"UTC\",\n",
        "        \"spark.sql.shuffle.partitions\": \"4\",\n",
        "        \"spark.default.parallelism\": \"4\",\n",
        "        # Executor resource settings - keep low to avoid OOM\n",
        "        \"spark.executor.memory\": \"1g\",\n",
        "        \"spark.executor.cores\": \"1\",\n",
        "        \"spark.driver.memory\": \"1g\",\n",
        "        # Limit number of executors\n",
        "        \"spark.executor.instances\": \"2\",\n",
        "    }\n",
        "    \n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        # Running on Darwin cluster - use distributed Spark via Ray\n",
        "        print(\"Mode: Darwin Cluster (Distributed)\")\n",
        "        print(\"Resource settings: 2 executors, 1 core each, 1GB memory each\")\n",
        "        ray.init()\n",
        "        spark = init_spark_with_configs(spark_configs=spark_configs)\n",
        "        \n",
        "        # Install packages on executor nodes\n",
        "        install_packages_on_executors(spark)\n",
        "    else:\n",
        "        # Running locally - use local Spark session\n",
        "        print(\"Mode: Local Spark Session\")\n",
        "        builder = SparkSession.builder \\\n",
        "            .appName(\"Iris-RandomForest-Spark-Local\") \\\n",
        "            .master(\"local[*]\")\n",
        "        \n",
        "        for key, value in spark_configs.items():\n",
        "            builder = builder.config(key, value)\n",
        "        \n",
        "        spark = builder.getOrCreate()\n",
        "    \n",
        "    print(f\"Spark version: {spark.version}\")\n",
        "    print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "    \n",
        "    return spark\n",
        "\n",
        "\n",
        "def cleanup_spark(spark):\n",
        "    \"\"\"Stop Spark session properly based on environment.\"\"\"\n",
        "    print(\"\\nStopping Spark session...\")\n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        stop_spark()\n",
        "    else:\n",
        "        spark.stop()\n",
        "    print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_mlflow(mlflow_uri: str, username: str, password: str) -> MlflowClient:\n",
        "    \"\"\"Configure MLflow tracking and return client.\"\"\"\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = username\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = password\n",
        "    \n",
        "    set_tracking_uri(mlflow_uri)\n",
        "    client = MlflowClient(mlflow_uri)\n",
        "    \n",
        "    print(f\"MLflow tracking URI: {mlflow_uri}\")\n",
        "    return client\n",
        "\n",
        "\n",
        "def load_and_prepare_data(spark: SparkSession):\n",
        "    \"\"\"Load Iris dataset and prepare Spark DataFrames.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load dataset as pandas\n",
        "    iris = load_iris(as_frame=True)\n",
        "    pdf = iris.data.copy()\n",
        "    pdf.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "    pdf['label'] = iris.target.astype(float)  # Spark ML requires double type for labels\n",
        "    \n",
        "    target_names = iris.target_names.tolist()\n",
        "    \n",
        "    print(f\"Dataset: Iris\")\n",
        "    print(f\"Samples: {len(pdf):,}\")\n",
        "    print(f\"Features: {len(iris.feature_names)}\")\n",
        "    \n",
        "    print(f\"\\nFeature names:\")\n",
        "    for i, col_name in enumerate(pdf.columns[:-1], 1):\n",
        "        print(f\"  {i}. {col_name}\")\n",
        "    \n",
        "    print(f\"\\nTarget classes:\")\n",
        "    for i, name in enumerate(target_names):\n",
        "        count = (pdf['label'] == i).sum()\n",
        "        print(f\"  {i}. {name}: {count} samples\")\n",
        "    \n",
        "    # Convert to Spark DataFrame\n",
        "    print(\"\\nConverting to Spark DataFrame...\")\n",
        "    df = spark.createDataFrame(pdf)\n",
        "    \n",
        "    # Get feature column names (all except label)\n",
        "    feature_cols = [c for c in df.columns if c != 'label']\n",
        "    \n",
        "    # Assemble features into vector column\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    \n",
        "    # Split data (stratified-like split using randomSplit)\n",
        "    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "    \n",
        "    train_count = train_df.count()\n",
        "    test_count = test_df.count()\n",
        "    \n",
        "    print(f\"\\nTrain samples: {train_count:,}\")\n",
        "    print(f\"Test samples: {test_count:,}\")\n",
        "    print(f\"Spark partitions: {train_df.rdd.getNumPartitions()}\")\n",
        "    \n",
        "    return train_df, test_df, feature_cols, target_names\n",
        "\n",
        "\n",
        "def train_model(train_df, test_df, hyperparams: dict):\n",
        "    \"\"\"Train Random Forest model using PySpark ML (distributed training).\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING MODEL (Distributed on Spark)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in hyperparams.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Create Random Forest Classifier for distributed training\n",
        "    rf_classifier = SparkRFClassifier(\n",
        "        featuresCol=\"features\",\n",
        "        labelCol=\"label\",\n",
        "        predictionCol=\"prediction\",\n",
        "        probabilityCol=\"probability\",\n",
        "        rawPredictionCol=\"rawPrediction\",\n",
        "        numTrees=hyperparams.get(\"n_estimators\", 100),\n",
        "        maxDepth=hyperparams.get(\"max_depth\", 10),\n",
        "        minInstancesPerNode=hyperparams.get(\"min_samples_leaf\", 1),\n",
        "        seed=hyperparams.get(\"random_state\", 42),\n",
        "    )\n",
        "    \n",
        "    # Train model (distributed across Spark executors)\n",
        "    print(\"\\nTraining distributed Random Forest model...\")\n",
        "    model = rf_classifier.fit(train_df)\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    print(f\"Number of trees: {model.getNumTrees}\")\n",
        "    \n",
        "    # Make predictions\n",
        "    train_pred = model.transform(train_df)\n",
        "    test_pred = model.transform(test_df)\n",
        "    \n",
        "    return model, train_pred, test_pred\n",
        "\n",
        "\n",
        "def calculate_metrics_spark(predictions_df, label_col=\"label\", prediction_col=\"prediction\", dataset_name=\"Test\"):\n",
        "    \"\"\"Calculate evaluation metrics using Spark's MulticlassClassificationEvaluator.\"\"\"\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=prediction_col)\n",
        "    \n",
        "    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions_df)\n",
        "    f1 = evaluator.setMetricName(\"f1\").evaluate(predictions_df)\n",
        "    precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions_df)\n",
        "    recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions_df)\n",
        "    \n",
        "    return {\n",
        "        f\"{dataset_name.lower()}_accuracy\": accuracy,\n",
        "        f\"{dataset_name.lower()}_precision\": precision,\n",
        "        f\"{dataset_name.lower()}_recall\": recall,\n",
        "        f\"{dataset_name.lower()}_f1\": f1\n",
        "    }\n",
        "\n",
        "\n",
        "def log_to_mlflow(model, train_df, train_pred, test_pred, hyperparams, feature_names, target_names):\n",
        "    \"\"\"Log model, parameters, and metrics to MLflow.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOGGING TO MLFLOW\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Log hyperparameters\n",
        "    for key, value in hyperparams.items():\n",
        "        mlflow.log_param(key, value)\n",
        "    \n",
        "    # Log additional Spark-specific params\n",
        "    mlflow.log_param(\"training_framework\", \"spark_ml\")\n",
        "    mlflow.log_param(\"distributed\", True)\n",
        "    mlflow.log_param(\"num_trees\", model.getNumTrees)\n",
        "    \n",
        "    # Calculate and log metrics\n",
        "    train_metrics = calculate_metrics_spark(train_pred, dataset_name=\"Train\")\n",
        "    test_metrics = calculate_metrics_spark(test_pred, dataset_name=\"Test\")\n",
        "    all_metrics = {**train_metrics, **test_metrics}\n",
        "    \n",
        "    for metric_name, metric_value in all_metrics.items():\n",
        "        mlflow.log_metric(metric_name, metric_value)\n",
        "    \n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"  Training Accuracy: {train_metrics['train_accuracy']:.4f}\")\n",
        "    print(f\"  Training F1: {train_metrics['train_f1']:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_metrics['test_accuracy']:.4f}\")\n",
        "    print(f\"  Test Precision: {test_metrics['test_precision']:.4f}\")\n",
        "    print(f\"  Test Recall: {test_metrics['test_recall']:.4f}\")\n",
        "    print(f\"  Test F1: {test_metrics['test_f1']:.4f}\")\n",
        "    \n",
        "    # Print confusion matrix\n",
        "    test_pdf = test_pred.select(\"label\", \"prediction\").toPandas()\n",
        "    cm = confusion_matrix(test_pdf[\"label\"], test_pdf[\"prediction\"])\n",
        "    print(f\"\\n  Confusion Matrix:\")\n",
        "    print(f\"  {cm}\")\n",
        "    \n",
        "    # Create sample input for reference\n",
        "    sample_input = train_df.select(feature_names).limit(1).toPandas()\n",
        "    \n",
        "    # Try to log model artifacts\n",
        "    model_logged = False\n",
        "    print(\"\\nSaving model artifacts...\")\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            # Save the Spark ML model\n",
        "            model_path = os.path.join(tmpdir, \"spark_model\")\n",
        "            model.save(model_path)\n",
        "            \n",
        "            # Save input example as JSON\n",
        "            input_example_file = os.path.join(tmpdir, \"input_example.json\")\n",
        "            sample_input.to_json(input_example_file, orient=\"records\", indent=2)\n",
        "            \n",
        "            # Save target names\n",
        "            target_names_file = os.path.join(tmpdir, \"target_names.json\")\n",
        "            with open(target_names_file, \"w\") as f:\n",
        "                json.dump(target_names, f)\n",
        "            \n",
        "            # Log artifacts\n",
        "            mlflow.log_artifacts(model_path, artifact_path=\"model/spark_model\")\n",
        "            mlflow.log_artifact(input_example_file, artifact_path=\"model\")\n",
        "            mlflow.log_artifact(target_names_file, artifact_path=\"model\")\n",
        "            \n",
        "            model_logged = True\n",
        "            print(\"Model artifacts logged successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log model artifacts: {e}\")\n",
        "        print(\"Metrics and parameters were logged successfully.\")\n",
        "    \n",
        "    # Store references for later use\n",
        "    all_metrics[\"_spark_model\"] = model\n",
        "    all_metrics[\"_model_logged\"] = model_logged\n",
        "    \n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "def register_model(client: MlflowClient, model_name: str, run_id: str, experiment_id: str):\n",
        "    \"\"\"Register model in MLflow Model Registry.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"REGISTERING MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "    \n",
        "    # Create registered model if it doesn't exist\n",
        "    try:\n",
        "        client.get_registered_model(model_name)\n",
        "        print(f\"Model '{model_name}' already exists in registry\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            client.create_registered_model(model_name)\n",
        "            print(f\"Created registered model: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create registered model: {e}\")\n",
        "    \n",
        "    # Create model version\n",
        "    try:\n",
        "        result = client.create_model_version(\n",
        "            name=model_name,\n",
        "            source=model_uri,\n",
        "            run_id=run_id\n",
        "        )\n",
        "        print(f\"Model version registered successfully!\")\n",
        "        print(f\"   Model Name: {model_name}\")\n",
        "        print(f\"   Version: {result.version}\")\n",
        "        print(f\"   Run ID: {run_id}\")\n",
        "        return result.version\n",
        "    except Exception as e:\n",
        "        print(f\"Model registration failed (model still usable via run URI): {e}\")\n",
        "        print(f\"   You can deploy using: mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_model_and_predict(sample_data: pd.DataFrame, spark_model, spark: SparkSession, feature_names: list, target_names: list):\n",
        "    \"\"\"Load model and run prediction on driver.\n",
        "    \n",
        "    Args:\n",
        "        sample_data: Pandas DataFrame with feature data\n",
        "        spark_model: The Spark ML Random Forest model\n",
        "        spark: SparkSession for creating DataFrame\n",
        "        feature_names: List of feature column names\n",
        "        target_names: List of target class names\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING MODEL AND RUNNING PREDICTION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if spark_model is None:\n",
        "        print(\"Error: No model provided\")\n",
        "        return None, None\n",
        "    \n",
        "    print(\"Using in-memory Spark model for prediction\")\n",
        "    \n",
        "    # Create Spark DataFrame from sample\n",
        "    sample_spark_df = spark.createDataFrame(sample_data)\n",
        "    \n",
        "    # Assemble features\n",
        "    assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n",
        "    sample_spark_df = assembler.transform(sample_spark_df)\n",
        "    \n",
        "    # Run prediction\n",
        "    predictions_df = spark_model.transform(sample_spark_df)\n",
        "    \n",
        "    # Extract results\n",
        "    result = predictions_df.select(\"prediction\", \"probability\").collect()[0]\n",
        "    prediction = int(result[\"prediction\"])\n",
        "    probabilities = result[\"probability\"].toArray()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAMPLE PREDICTION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nInput features:\")\n",
        "    for col_name in feature_names:\n",
        "        print(f\"  {col_name}: {sample_data[col_name].iloc[0]:.4f}\")\n",
        "    \n",
        "    print(f\"\\nClass Probabilities:\")\n",
        "    for i, prob in enumerate(probabilities):\n",
        "        species = target_names[i] if i < len(target_names) else f\"Class {i}\"\n",
        "        print(f\"  {species}: {prob:.4f}\")\n",
        "    \n",
        "    predicted_species = target_names[prediction] if prediction < len(target_names) else f\"Class {prediction}\"\n",
        "    print(f\"\\nPredicted Class: {prediction} ({predicted_species})\")\n",
        "    \n",
        "    return prediction, probabilities\n",
        "\n",
        "\n",
        "def print_deployment_info(run_id: str, experiment_id: str, model_name: str, model_version: str):\n",
        "    \"\"\"Print deployment instructions and sample payloads.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nRun Information:\")\n",
        "    print(f\"  Run ID: {run_id}\")\n",
        "    print(f\"  Experiment ID: {experiment_id}\")\n",
        "    print(f\"  Model URI (run): runs:/{run_id}/model\")\n",
        "    if model_version:\n",
        "        print(f\"  Model URI (registry): models:/{model_name}/{model_version}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DEPLOYMENT PAYLOAD (deploy-model API)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    deploy_payload = {\n",
        "        \"serve_name\": \"iris-rf-spark-classifier\",\n",
        "        \"model_uri\": f\"mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\",\n",
        "        \"env\": \"local\",\n",
        "        \"cores\": 2,\n",
        "        \"memory\": 4,\n",
        "        \"node_capacity\": \"spot\",\n",
        "        \"min_replicas\": 1,\n",
        "        \"max_replicas\": 3\n",
        "    }\n",
        "    \n",
        "    print(json.dumps(deploy_payload, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train Iris Classification Model on Spark\")\n",
        "    parser.add_argument(\n",
        "        \"--mlflow-uri\",\n",
        "        default=\"http://darwin-mlflow-lib.darwin.svc.cluster.local:8080\",\n",
        "        help=\"MLflow tracking URI\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--username\",\n",
        "        default=\"abc@gmail.com\",\n",
        "        help=\"MLflow username\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--password\",\n",
        "        default=\"password\",\n",
        "        help=\"MLflow password\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--experiment-name\",\n",
        "        default=\"iris_spark_classification\",\n",
        "        help=\"MLflow experiment name\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model-name\",\n",
        "        default=\"IrisSparkRFClassifier\",\n",
        "        help=\"Registered model name\"\n",
        "    )\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"IRIS SPECIES CLASSIFICATION WITH RANDOM FOREST ON SPARK\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Initialize Spark\n",
        "    spark = initialize_spark()\n",
        "    \n",
        "    # Setup MLflow\n",
        "    client = setup_mlflow(args.mlflow_uri, args.username, args.password)\n",
        "    set_experiment(experiment_name=args.experiment_name)\n",
        "    print(f\"Experiment: {args.experiment_name}\")\n",
        "    \n",
        "    # Load data\n",
        "    train_df, test_df, feature_names, target_names = load_and_prepare_data(spark)\n",
        "    \n",
        "    # Define hyperparameters\n",
        "    hyperparams = {\n",
        "        \"n_estimators\": 100,\n",
        "        \"max_depth\": 10,\n",
        "        \"min_samples_leaf\": 1,\n",
        "        \"random_state\": 42,\n",
        "    }\n",
        "    \n",
        "    # Start MLflow run\n",
        "    with mlflow.start_run(run_name=f\"spark_rf_iris_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "        # Train model (distributed on Spark)\n",
        "        model, train_pred, test_pred = train_model(\n",
        "            train_df, test_df, hyperparams\n",
        "        )\n",
        "        \n",
        "        # Log to MLflow\n",
        "        metrics = log_to_mlflow(\n",
        "            model, train_df, train_pred, test_pred, hyperparams, feature_names, target_names\n",
        "        )\n",
        "        \n",
        "        # Get run information\n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "        experiment_id = mlflow.active_run().info.experiment_id\n",
        "    \n",
        "    # Register model (outside of run context) - only if artifacts were logged\n",
        "    model_version = None\n",
        "    if metrics.get(\"_model_logged\", False):\n",
        "        model_version = register_model(client, args.model_name, run_id, experiment_id)\n",
        "    else:\n",
        "        print(\"\\nSkipping model registration (artifacts not logged to MLflow)\")\n",
        "    \n",
        "    # Demonstrate running inference on driver\n",
        "    sample_pdf = test_df.select(feature_names).limit(1).toPandas()\n",
        "    spark_model = metrics.get(\"_spark_model\")\n",
        "    prediction, probabilities = load_model_and_predict(\n",
        "        sample_pdf, spark_model, spark, feature_names, target_names\n",
        "    )\n",
        "    \n",
        "    # Get actual value for comparison\n",
        "    actual_value = test_df.select(\"label\").limit(1).toPandas()[\"label\"].iloc[0]\n",
        "    actual_species = target_names[int(actual_value)] if int(actual_value) < len(target_names) else f\"Class {int(actual_value)}\"\n",
        "    print(f\"\\nActual Class: {int(actual_value)} ({actual_species})\")\n",
        "    print(f\"Prediction Correct: {prediction == int(actual_value)}\")\n",
        "    \n",
        "    # Print deployment information\n",
        "    print_deployment_info(run_id, experiment_id, args.model_name, model_version)\n",
        "    \n",
        "    # Cleanup: Stop Spark session\n",
        "    cleanup_spark(spark)\n",
        "    \n",
        "    print(\"\\nScript completed successfully!\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
