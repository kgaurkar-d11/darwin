{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diabetes Progression Prediction with XGBoost on Spark\n",
        "\n",
        "This example demonstrates **distributed training** of an XGBoost regressor using Spark to predict diabetes disease progression.\n",
        "\n",
        "## Execution Modes\n",
        "\n",
        "This notebook supports two execution modes:\n",
        "- **Darwin Cluster Mode**: Uses Darwin SDK with Ray for distributed Spark processing (automatic when running on Darwin cluster)\n",
        "- **Local Mode**: Uses local Spark session for development/testing (automatic fallback when Darwin SDK is not available)\n",
        "\n",
        "## Dataset\n",
        "\n",
        "- **Name**: Diabetes Dataset\n",
        "- **Samples**: 442\n",
        "- **Features**: 10 baseline variables\n",
        "- **Target**: Quantitative measure of disease progression one year after baseline\n",
        "- **Type**: Regression\n",
        "\n",
        "## Model\n",
        "\n",
        "- **Framework**: XGBoost with Spark (SparkXGBRegressor)\n",
        "- **Type**: Distributed Gradient Boosting Regressor\n",
        "- **Objective**: Squared error regression\n",
        "\n",
        "## Features\n",
        "\n",
        "The dataset includes 10 baseline variables:\n",
        "- `age`: Age in years\n",
        "- `sex`: Sex\n",
        "- `bmi`: Body mass index\n",
        "- `bp`: Average blood pressure\n",
        "- `s1`: Total serum cholesterol\n",
        "- `s2`: Low-density lipoproteins\n",
        "- `s3`: High-density lipoproteins\n",
        "- `s4`: Total cholesterol / HDL\n",
        "- `s5`: Log of serum triglycerides level\n",
        "- `s6`: Blood sugar level\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Training uses `SparkXGBRegressor` for Spark-based XGBoost training\n",
        "- Data is processed as Spark DataFrames with `VectorAssembler`\n",
        "- Model is registered to MLflow Model Registry for versioning\n",
        "- Demonstrates loading model from MLflow URI and running inference\n",
        "- Compares predicted vs actual values for validation\n",
        "- **Auto-installs xgboost on Spark executors** (required for distributed training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix pyOpenSSL/cryptography compatibility issue first\n",
        "%pip install --upgrade pyOpenSSL cryptography\n",
        "\n",
        "# Install main dependencies\n",
        "%pip install xgboost>=2.0.0 pandas numpy scikit-learn mlflow pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# XGBoost imports\n",
        "import xgboost as xgb\n",
        "from xgboost.spark import SparkXGBRegressor\n",
        "\n",
        "# Spark imports\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# MLflow imports\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from mlflow import set_tracking_uri, set_experiment\n",
        "from mlflow.client import MlflowClient\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "# Scikit-learn imports (for loading dataset)\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "# Darwin SDK imports (optional - only available on Darwin cluster)\n",
        "DARWIN_SDK_AVAILABLE = False\n",
        "try:\n",
        "    import ray\n",
        "    from darwin import init_spark_with_configs, stop_spark\n",
        "    DARWIN_SDK_AVAILABLE = True\n",
        "    print(\"Darwin SDK available - will use distributed Spark on Darwin cluster\")\n",
        "except ImportError as e:\n",
        "    print(f\"Darwin SDK not available: {e}\")\n",
        "    print(\"Running in LOCAL mode - will use local Spark session\")\n",
        "except AttributeError as e:\n",
        "    # This typically happens with pyOpenSSL/cryptography version mismatch\n",
        "    if \"X509_V_FLAG\" in str(e) or \"lib\" in str(e):\n",
        "        print(\"=\" * 80)\n",
        "        print(\"ERROR: pyOpenSSL/cryptography version conflict detected!\")\n",
        "        print(\"Please run the following in a cell before importing:\")\n",
        "        print(\"  %pip install --upgrade pyOpenSSL cryptography\")\n",
        "        print(\"Then restart the kernel and try again.\")\n",
        "        print(\"=\" * 80)\n",
        "        raise\n",
        "    else:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def install_packages_on_executors(spark):\n",
        "    \"\"\"Install required packages on Spark executors using pip.\"\"\"\n",
        "    print(\"Installing packages on Spark executors...\")\n",
        "    \n",
        "    # Packages required for xgboost.spark on executors\n",
        "    EXECUTOR_PACKAGES = [\n",
        "        \"xgboost>=2.0.0\",\n",
        "        \"scikit-learn\",\n",
        "        \"pandas\",\n",
        "        \"numpy\",\n",
        "    ]\n",
        "    \n",
        "    def install_packages(iterator):\n",
        "        import subprocess\n",
        "        import sys\n",
        "        # Install all required packages on each executor\n",
        "        subprocess.check_call([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", \n",
        "            *EXECUTOR_PACKAGES,\n",
        "            \"-q\", \"--disable-pip-version-check\"\n",
        "        ])\n",
        "        yield True\n",
        "    \n",
        "    # Run installation on all executors\n",
        "    num_executors = spark.sparkContext.defaultParallelism\n",
        "    spark.sparkContext.parallelize(range(num_executors), num_executors) \\\n",
        "        .mapPartitions(install_packages) \\\n",
        "        .collect()\n",
        "    \n",
        "    print(f\"Packages installed on {num_executors} executors: {', '.join(EXECUTOR_PACKAGES)}\")\n",
        "\n",
        "\n",
        "def initialize_spark():\n",
        "    \"\"\"Initialize Spark session - uses Darwin SDK on cluster, local Spark otherwise.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"INITIALIZING SPARK SESSION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Spark configurations\n",
        "    spark_configs = {\n",
        "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
        "        \"spark.sql.session.timeZone\": \"UTC\",\n",
        "        \"spark.sql.shuffle.partitions\": \"10\",\n",
        "        \"spark.default.parallelism\": \"10\",\n",
        "    }\n",
        "    \n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        # Running on Darwin cluster - use distributed Spark via Ray\n",
        "        print(\"Mode: Darwin Cluster (Distributed)\")\n",
        "        ray.init()\n",
        "        spark = init_spark_with_configs(spark_configs=spark_configs)\n",
        "        \n",
        "        # Install xgboost on executor nodes\n",
        "        install_packages_on_executors(spark)\n",
        "    else:\n",
        "        # Running locally - use local Spark session\n",
        "        print(\"Mode: Local Spark Session\")\n",
        "        builder = SparkSession.builder \\\n",
        "            .appName(\"XGBoost-Diabetes-Spark-Local\") \\\n",
        "            .master(\"local[*]\")\n",
        "        \n",
        "        for key, value in spark_configs.items():\n",
        "            builder = builder.config(key, value)\n",
        "        \n",
        "        spark = builder.getOrCreate()\n",
        "    \n",
        "    print(f\"Spark version: {spark.version}\")\n",
        "    print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
        "    \n",
        "    return spark\n",
        "\n",
        "\n",
        "def cleanup_spark(spark):\n",
        "    \"\"\"Stop Spark session properly based on environment.\"\"\"\n",
        "    print(\"\\nStopping Spark session...\")\n",
        "    if DARWIN_SDK_AVAILABLE:\n",
        "        stop_spark()\n",
        "    else:\n",
        "        spark.stop()\n",
        "    print(\"Spark session stopped.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_mlflow(mlflow_uri: str, username: str, password: str) -> MlflowClient:\n",
        "    \"\"\"Configure MLflow tracking and return client.\"\"\"\n",
        "    os.environ[\"MLFLOW_TRACKING_USERNAME\"] = username\n",
        "    os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = password\n",
        "    \n",
        "    set_tracking_uri(mlflow_uri)\n",
        "    client = MlflowClient(mlflow_uri)\n",
        "    \n",
        "    print(f\"MLflow tracking URI: {mlflow_uri}\")\n",
        "    return client\n",
        "\n",
        "\n",
        "def load_and_prepare_data(spark: SparkSession):\n",
        "    \"\"\"Load Diabetes dataset and prepare Spark DataFrames.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load dataset as pandas\n",
        "    data = load_diabetes(as_frame=True)\n",
        "    pdf = data.data.copy()\n",
        "    pdf['target'] = data.target\n",
        "    \n",
        "    print(f\"Dataset: Diabetes\")\n",
        "    print(f\"Samples: {len(pdf):,}\")\n",
        "    print(f\"Features: {len(data.feature_names)}\")\n",
        "    \n",
        "    print(f\"\\nFeature names:\")\n",
        "    for i, col in enumerate(data.feature_names, 1):\n",
        "        print(f\"  {i}. {col}\")\n",
        "    \n",
        "    print(f\"\\nTarget statistics:\")\n",
        "    print(f\"  Mean: {pdf['target'].mean():.2f}\")\n",
        "    print(f\"  Std: {pdf['target'].std():.2f}\")\n",
        "    print(f\"  Min: {pdf['target'].min():.2f}\")\n",
        "    print(f\"  Max: {pdf['target'].max():.2f}\")\n",
        "    \n",
        "    # Convert to Spark DataFrame\n",
        "    print(\"\\nConverting to Spark DataFrame...\")\n",
        "    df = spark.createDataFrame(pdf)\n",
        "    \n",
        "    # Get feature column names (all except target)\n",
        "    feature_cols = [c for c in df.columns if c != 'target']\n",
        "    \n",
        "    # Assemble features into vector column\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "    df = assembler.transform(df)\n",
        "    \n",
        "    # Split data\n",
        "    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
        "    \n",
        "    train_count = train_df.count()\n",
        "    test_count = test_df.count()\n",
        "    \n",
        "    print(f\"\\nTrain samples: {train_count:,}\")\n",
        "    print(f\"Test samples: {test_count:,}\")\n",
        "    print(f\"Spark partitions: {train_df.rdd.getNumPartitions()}\")\n",
        "    \n",
        "    return train_df, test_df, feature_cols\n",
        "\n",
        "\n",
        "def train_model(train_df, test_df, hyperparams: dict):\n",
        "    \"\"\"Train XGBoost model using SparkXGBRegressor (distributed training).\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING MODEL (Distributed on Spark)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"Hyperparameters:\")\n",
        "    for key, value in hyperparams.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Create SparkXGBRegressor for distributed training\n",
        "    xgb_regressor = SparkXGBRegressor(\n",
        "        features_col=\"features\",\n",
        "        label_col=\"target\",\n",
        "        prediction_col=\"prediction\",\n",
        "        objective=\"reg:squarederror\",\n",
        "        max_depth=hyperparams.get(\"max_depth\", 5),\n",
        "        learning_rate=hyperparams.get(\"learning_rate\", 0.1),\n",
        "        n_estimators=hyperparams.get(\"n_estimators\", 100),\n",
        "        subsample=hyperparams.get(\"subsample\", 0.8),\n",
        "        colsample_bytree=hyperparams.get(\"colsample_bytree\", 0.8),\n",
        "        random_state=hyperparams.get(\"random_state\", 42),\n",
        "    )\n",
        "    \n",
        "    # Train model (distributed across Spark executors)\n",
        "    print(\"\\nTraining distributed XGBoost model...\")\n",
        "    model = xgb_regressor.fit(train_df)\n",
        "    \n",
        "    print(\"Training completed!\")\n",
        "    \n",
        "    # Make predictions\n",
        "    train_pred = model.transform(train_df)\n",
        "    test_pred = model.transform(test_df)\n",
        "    \n",
        "    return model, train_pred, test_pred\n",
        "\n",
        "\n",
        "def calculate_metrics(predictions_df, label_col=\"target\", prediction_col=\"prediction\", dataset_name=\"Test\"):\n",
        "    \"\"\"Calculate evaluation metrics using Spark's RegressionEvaluator.\"\"\"\n",
        "    evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=prediction_col)\n",
        "    \n",
        "    rmse = evaluator.setMetricName(\"rmse\").evaluate(predictions_df)\n",
        "    mae = evaluator.setMetricName(\"mae\").evaluate(predictions_df)\n",
        "    r2 = evaluator.setMetricName(\"r2\").evaluate(predictions_df)\n",
        "    \n",
        "    return {\n",
        "        f\"{dataset_name.lower()}_rmse\": rmse,\n",
        "        f\"{dataset_name.lower()}_mae\": mae,\n",
        "        f\"{dataset_name.lower()}_r2\": r2\n",
        "    }\n",
        "\n",
        "\n",
        "def log_to_mlflow(model, train_df, train_pred, test_pred, hyperparams, feature_names):\n",
        "    \"\"\"Log model, parameters, and metrics to MLflow.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOGGING TO MLFLOW\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Log hyperparameters\n",
        "    for key, value in hyperparams.items():\n",
        "        mlflow.log_param(key, value)\n",
        "    \n",
        "    # Log additional Spark-specific params\n",
        "    mlflow.log_param(\"training_framework\", \"spark_xgboost\")\n",
        "    mlflow.log_param(\"distributed\", True)\n",
        "    \n",
        "    # Calculate and log metrics\n",
        "    train_metrics = calculate_metrics(train_pred, dataset_name=\"Train\")\n",
        "    test_metrics = calculate_metrics(test_pred, dataset_name=\"Test\")\n",
        "    all_metrics = {**train_metrics, **test_metrics}\n",
        "    \n",
        "    for metric_name, metric_value in all_metrics.items():\n",
        "        mlflow.log_metric(metric_name, metric_value)\n",
        "    \n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"  Training RMSE: {train_metrics['train_rmse']:.4f}\")\n",
        "    print(f\"  Training R2: {train_metrics['train_r2']:.4f}\")\n",
        "    print(f\"  Test RMSE: {test_metrics['test_rmse']:.4f}\")\n",
        "    print(f\"  Test MAE: {test_metrics['test_mae']:.4f}\")\n",
        "    print(f\"  Test R2: {test_metrics['test_r2']:.4f}\")\n",
        "    \n",
        "    # Get native XGBoost model (Booster) from Spark model\n",
        "    native_model = model.get_booster()\n",
        "    \n",
        "    # Create sample input for signature (pandas DataFrame)\n",
        "    sample_input = train_df.select(feature_names).limit(1).toPandas()\n",
        "    \n",
        "    # Try to log model artifacts - with fallback for server issues\n",
        "    model_logged = False\n",
        "    \n",
        "    # Approach 1: Try simple model file upload (most reliable)\n",
        "    print(\"\\nSaving model artifacts...\")\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            # Save just the XGBoost model file (minimal, most compatible)\n",
        "            model_file = os.path.join(tmpdir, \"model.json\")\n",
        "            native_model.save_model(model_file)\n",
        "            \n",
        "            # Save input example as JSON\n",
        "            input_example_file = os.path.join(tmpdir, \"input_example.json\")\n",
        "            sample_input.to_json(input_example_file, orient=\"records\", indent=2)\n",
        "            \n",
        "            # Log individual files\n",
        "            mlflow.log_artifact(model_file, artifact_path=\"model\")\n",
        "            mlflow.log_artifact(input_example_file, artifact_path=\"model\")\n",
        "            \n",
        "            model_logged = True\n",
        "            print(\"Model artifacts logged successfully (minimal format)!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not log model artifacts: {e}\")\n",
        "        print(\"Metrics and parameters were logged successfully.\")\n",
        "        print(\"You can save the model locally and upload manually if needed.\")\n",
        "    \n",
        "    # Store native model reference for later use\n",
        "    all_metrics[\"_native_model\"] = native_model\n",
        "    all_metrics[\"_model_logged\"] = model_logged\n",
        "    \n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "def register_model(client: MlflowClient, model_name: str, run_id: str, experiment_id: str):\n",
        "    \"\"\"Register model in MLflow Model Registry.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"REGISTERING MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "    \n",
        "    # Create registered model if it doesn't exist\n",
        "    try:\n",
        "        client.get_registered_model(model_name)\n",
        "        print(f\"Model '{model_name}' already exists in registry\")\n",
        "    except Exception:\n",
        "        try:\n",
        "            client.create_registered_model(model_name)\n",
        "            print(f\"Created registered model: {model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create registered model: {e}\")\n",
        "    \n",
        "    # Create model version\n",
        "    try:\n",
        "        result = client.create_model_version(\n",
        "            name=model_name,\n",
        "            source=model_uri,\n",
        "            run_id=run_id\n",
        "        )\n",
        "        print(f\"Model version registered successfully!\")\n",
        "        print(f\"   Model Name: {model_name}\")\n",
        "        print(f\"   Version: {result.version}\")\n",
        "        print(f\"   Run ID: {run_id}\")\n",
        "        return result.version\n",
        "    except Exception as e:\n",
        "        print(f\"Model registration failed (model still usable via run URI): {e}\")\n",
        "        print(f\"   You can deploy using: mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_model_and_predict(model_uri: str, sample_data: pd.DataFrame, native_model=None):\n",
        "    \"\"\"Load model from MLflow URI and run prediction.\n",
        "    \n",
        "    Args:\n",
        "        model_uri: MLflow model URI (e.g., runs:/{run_id}/model)\n",
        "        sample_data: Pandas DataFrame with feature data\n",
        "        native_model: Optional - use this model directly instead of loading from MLflow\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"LOADING MODEL AND RUNNING PREDICTION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if native_model is not None:\n",
        "        # Use the native model directly (no need to load from MLflow)\n",
        "        print(\"Using in-memory model for prediction\")\n",
        "        loaded_model = native_model\n",
        "    else:\n",
        "        # Try to load from MLflow\n",
        "        print(f\"Model URI: {model_uri}\")\n",
        "        try:\n",
        "            # Try MLflow's xgboost loader first\n",
        "            loaded_model = mlflow.xgboost.load_model(model_uri)\n",
        "            print(\"Model loaded from MLflow successfully!\")\n",
        "        except Exception as e:\n",
        "            # Fallback: Download artifact and load manually\n",
        "            print(f\"MLflow loader failed, trying artifact download: {e}\")\n",
        "            try:\n",
        "                client = MlflowClient()\n",
        "                run_id = model_uri.split(\"/\")[1] if model_uri.startswith(\"runs:/\") else model_uri\n",
        "                \n",
        "                with tempfile.TemporaryDirectory() as tmpdir:\n",
        "                    local_path = client.download_artifacts(run_id, \"model/model.json\", tmpdir)\n",
        "                    loaded_model = xgb.Booster()\n",
        "                    loaded_model.load_model(local_path)\n",
        "                    print(\"Model loaded from artifact successfully!\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Could not load model: {e2}\")\n",
        "                raise\n",
        "    \n",
        "    # Create DMatrix for prediction\n",
        "    dmatrix = xgb.DMatrix(sample_data)\n",
        "    \n",
        "    # Run prediction\n",
        "    predictions = loaded_model.predict(dmatrix)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SAMPLE PREDICTION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nInput features:\")\n",
        "    for col in sample_data.columns:\n",
        "        print(f\"  {col}: {sample_data[col].iloc[0]:.4f}\")\n",
        "    print(f\"\\nPredicted diabetes progression: {predictions[0]:.2f}\")\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "\n",
        "def print_deployment_info(run_id: str, experiment_id: str, model_name: str, model_version: str):\n",
        "    \"\"\"Print deployment instructions and sample payloads.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nRun Information:\")\n",
        "    print(f\"  Run ID: {run_id}\")\n",
        "    print(f\"  Experiment ID: {experiment_id}\")\n",
        "    print(f\"  Model URI (run): runs:/{run_id}/model\")\n",
        "    if model_version:\n",
        "        print(f\"  Model URI (registry): models:/{model_name}/{model_version}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DEPLOYMENT PAYLOAD (deploy-model API)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    deploy_payload = {\n",
        "        \"serve_name\": \"diabetes-xgboost-spark-regressor\",\n",
        "        \"model_uri\": f\"mlflow-artifacts:/{experiment_id}/{run_id}/artifacts/model\",\n",
        "        \"env\": \"local\",\n",
        "        \"cores\": 2,\n",
        "        \"memory\": 4,\n",
        "        \"node_capacity\": \"spot\",\n",
        "        \"min_replicas\": 1,\n",
        "        \"max_replicas\": 3\n",
        "    }\n",
        "    \n",
        "    print(json.dumps(deploy_payload, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Train XGBoost Diabetes Regression Model on Spark\")\n",
        "    parser.add_argument(\n",
        "        \"--mlflow-uri\",\n",
        "        default=\"http://darwin-mlflow-lib.darwin.svc.cluster.local:8080\",\n",
        "        help=\"MLflow tracking URI\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--username\",\n",
        "        default=\"abc@gmail.com\",\n",
        "        help=\"MLflow username\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--password\",\n",
        "        default=\"password\",\n",
        "        help=\"MLflow password\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--experiment-name\",\n",
        "        default=\"diabetes_xgboost_spark_regression\",\n",
        "        help=\"MLflow experiment name\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model-name\",\n",
        "        default=\"DiabetesXGBoostSparkRegressor\",\n",
        "        help=\"Registered model name\"\n",
        "    )\n",
        "    \n",
        "    args, _ = parser.parse_known_args()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"DIABETES PROGRESSION PREDICTION WITH XGBOOST ON SPARK\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Initialize Spark\n",
        "    spark = initialize_spark()\n",
        "    \n",
        "    # Setup MLflow\n",
        "    client = setup_mlflow(args.mlflow_uri, args.username, args.password)\n",
        "    set_experiment(experiment_name=args.experiment_name)\n",
        "    print(f\"Experiment: {args.experiment_name}\")\n",
        "    \n",
        "    # Load data\n",
        "    train_df, test_df, feature_names = load_and_prepare_data(spark)\n",
        "    \n",
        "    # Define hyperparameters\n",
        "    hyperparams = {\n",
        "        \"objective\": \"reg:squarederror\",\n",
        "        \"max_depth\": 5,\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"n_estimators\": 100,\n",
        "        \"subsample\": 0.8,\n",
        "        \"colsample_bytree\": 0.8,\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "    \n",
        "    # Start MLflow run\n",
        "    with mlflow.start_run(run_name=f\"spark_xgboost_diabetes_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "        # Train model (distributed on Spark)\n",
        "        model, train_pred, test_pred = train_model(\n",
        "            train_df, test_df, hyperparams\n",
        "        )\n",
        "        \n",
        "        # Log to MLflow\n",
        "        metrics = log_to_mlflow(\n",
        "            model, train_df, train_pred, test_pred, hyperparams, feature_names\n",
        "        )\n",
        "        \n",
        "        # Get run information\n",
        "        run_id = mlflow.active_run().info.run_id\n",
        "        experiment_id = mlflow.active_run().info.experiment_id\n",
        "    \n",
        "    # Register model (outside of run context) - only if artifacts were logged\n",
        "    model_version = None\n",
        "    if metrics.get(\"_model_logged\", False):\n",
        "        model_version = register_model(client, args.model_name, run_id, experiment_id)\n",
        "    else:\n",
        "        print(\"\\nSkipping model registration (artifacts not logged to MLflow)\")\n",
        "    \n",
        "    # Demonstrate running inference\n",
        "    model_uri = f\"runs:/{run_id}/model\"\n",
        "    sample_pdf = test_df.select(feature_names).limit(1).toPandas()\n",
        "    \n",
        "    # Use the native model for prediction (more reliable than loading from MLflow)\n",
        "    native_model = metrics.get(\"_native_model\")\n",
        "    predictions = load_model_and_predict(model_uri, sample_pdf, native_model=native_model)\n",
        "    \n",
        "    # Get actual value for comparison\n",
        "    actual_value = test_df.select(\"target\").limit(1).toPandas()[\"target\"].iloc[0]\n",
        "    print(f\"Actual diabetes progression: {actual_value:.2f}\")\n",
        "    print(f\"Prediction error: {abs(predictions[0] - actual_value):.2f}\")\n",
        "    \n",
        "    # Print deployment information\n",
        "    print_deployment_info(run_id, experiment_id, args.model_name, model_version)\n",
        "    \n",
        "    # Cleanup: Stop Spark session\n",
        "    cleanup_spark(spark)\n",
        "    \n",
        "    print(\"\\nScript completed successfully!\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
