[spark.configs]
spark.memory.fraction = 0.5
spark.memory.storageFraction = 0.2
spark.task.maxFailures = 4
spark.eventLog.enabled = true
spark.eventLog.logStageExecutorMetrics = true
spark.sql.adaptive.skewJoin.enabled = true
spark.sql.adaptive.skewJoin.skewedPartitionFactor = 3
spark.sql.adaptive.advisoryPartitionSizeInBytes = 128
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes = 256
spark.sql.adaptive.enabled = true
spark.sql.adaptive.coalescePartitions.enabled = true
spark.sql.adaptive.coalescePartitions.minPartitionSize = 25M
spark.sql.adaptive.forceOptimizeSkewedJoin = true
spark.sql.shuffle.partitions = 200
spark.ui.showConsoleProgress = false
spark.scheduler.mode = FAIR
spark.sql.parquet.int96RebaseModeInRead = CORRECTED
spark.local.dir = /tmp/spark
spark.hive.imetastoreclient.factory.class = com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
spark.hadoop.fs.s3.impl = org.apache.hadoop.fs.s3a.S3AFileSystem
spark.sql.sources.commitProtocolClass = org.apache.spark.internal.io.cloud.PathOutputCommitProtocol
spark.sql.parquet.output.committer.class = org.apache.hadoop.mapreduce.lib.output.BindingPathOutputCommitter
spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a = org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
spark.hadoop.fs.s3a.committer.name = magic
spark.hadoop.fs.s3a.committer.magic.enabled = true
spark.sql.extensions = io.delta.sql.DeltaSparkSessionExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog = org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.catalog.spark_catalog.warehouse = s3://org-lakehouse/iceberg/warehouse/
spark.sql.catalog.spark_catalog.catalog-impl = org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.spark_catalog.io-impl = org.apache.iceberg.aws.s3.S3FileIO
spark.hadoop.fs.s3a.aws.credentials.provider = com.amazonaws.auth.WebIdentityTokenCredentialsProvider
spark.ray.raydp_spark_master.actor.resource.ondemand = 1
spark.hadoop.fs.s3a.canned.acl = BucketOwnerFullControl
spark.hadoop.hive.metastore.glue.catalogid = <YOUR_AWS_ACCOUNT_ID>
spark.hadoop.fs.s3a.credentialsType = AssumeRole
spark.hadoop.fs.s3a.acl.default = BucketOwnerFullControl
spark.hadoop.fs.s3a.stsAssumeRole.arn = <YOUR_STS_ASSUME_ROLE_ARN>
spark.sql.catalog.iceberg_catalog.catalog-impl = org.apache.iceberg.aws.glue.GlueCatalog
spark.sql.catalog.iceberg_catalog.warehouse = s3://org-lakehouse/iceberg/warehouse/
spark.sql.catalog.iceberg_catalog.io-impl = org.apache.iceberg.aws.s3.S3FileIO
spark.sql.catalog.iceberg_catalog = org.apache.iceberg.spark.SparkCatalog
spark.sql.iceberg.handle-timestamp-without-timezone = true

[spark.jars]
iceberg_spark_runtime = iceberg-spark-runtime-3.5_2.12-1.5.2.jar
iceberg_aws_bundle = iceberg-aws-bundle-1.5.2.jar
hadoop_aws = hadoop-aws-3.3.4.jar
hadoop_common = hadoop-common-3.3.4.jar
aws_java_sdk_s3 = aws-java-sdk-s3-1.12.31.jar
spark_hadoop_cloud = spark-hadoop-cloud_2.12-3.5.0.jar
delta_spark = delta-spark_2.12-3.2.0.jar
aws_java_sdk_core = aws-java-sdk-core-1.12.31.jar
aws_java_sdk_dynamodb = aws-java-sdk-dynamodb-1.12.31.jar
aws_java_sdk_sts = aws-java-sdk-sts-1.12.31.jar
delta_storage = delta-storage-3.2.0.jar
spark_sql_kafka = spark-sql-kafka-0-10_2.12-3.5.0.jar
spark_token_provider_kafka = spark-token-provider-kafka-0-10_2.12-3.5.0.jar
kafka_clients = kafka-clients-3.4.1.jar
spark_avro = spark-avro_2.12-3.5.0.jar
spark_redshift = spark-redshift_2.12-6.3.0-spark_3.5.jar
redshift_jdbc = RedshiftJDBC42-no-awssdk-1.2.36.1060.jar

[spark.rss.configs]
spark.shuffle.manager = org.apache.spark.shuffle.celeborn.SparkShuffleManager
spark.serializer = org.apache.spark.serializer.KryoSerializer
spark.shuffle.service.enabled = false
celeborn.jar = celeborn-client-spark-3-shaded_2.12-0.5.0.jar
spark.celeborn.client.push.replicate.enabled = false
spark.shuffle.celeborn.fetch.maxRetries = 5
spark.shuffle.celeborn.fetch.retryWait = 10s

[spark.dynamicAllocation]
## Ref: https://github.com/apache/celeborn
spark.shuffle.service.enabled = false
spark.dynamicAllocation.enabled = true
spark.dynamicAllocation.minExecutors = 1
spark.dynamicAllocation.maxExecutors = 300
spark.dynamicAllocation.shuffleTracking.enabled = false
spark.sql.adaptive.enabled = true
spark.sql.adaptive.skewJoin.enabled = true
spark.sql.adaptive.localShuffleReader.enabled = false
spark.executor.userClassPathFirst = false