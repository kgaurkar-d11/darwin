variables: ##user defined reusable variables
  runtime: &runtime test_runtime
  terminate_after_minutes: &terminate_after_minutes 60
  spark_config: &spark_config
    spark.executor.memory: "4g"

  basic-packages: &basic-packages
   packages:
    - pypi: {'package': 'numpy==1.24.0', 'path': 'pypi_index_url'}
    - maven: {'package': 'org.apache.spark:spark-sql_2.12', 'version': '3.3.0', 'repository': 'spark'}
    - s3: {'path': 's3://my-bucket/libs/my-library.whl'}
    - workspace: {'path': '/Users/my_user/libs/utils.py'}


  advance_config1: &advance_config1
    spark_config:
      <<: *spark_config
    env_variables: ''
    init_script:
    - pip install some-package1

    ray_start_params:
      num_cpus_on_head: 0
      num_gpus_on_head: 0
      object_store_memory_perc: 25

  advance_config2: &advance_config2
      spark_config:
        <<: *spark_config
      env_variables: ''
      init_script:
      - pip install some-package2

      ray_start_params:
        num_cpus_on_head: 0
        num_gpus_on_head: 0
        object_store_memory_perc: 25

  advance_config3: &advance_config3
      spark_config:
        <<: *spark_config
      env_variables: ''
      init_script:
      - pip install some-package3
      ray_start_params:
        num_cpus_on_head: 1
        num_gpus_on_head: 1
        object_store_memory_perc: 2

  head_node_config1: &head_node_config1
      node:
        cores: 8
        memory: 16
        node_capacity_type: ondemand

  head_node_config2: &head_node_config2
      node:
        cores: 2
        memory: 10
        node_capacity_type: ondemand

  worker_node_config1: &worker_node_config1
      max_pods: 1
      min_pods: 1
      node:
        cores: 8
        memory: 16
        node_capacity_type: spot

  worker_node_config2: &worker_node_config2
      max_pods: 2
      min_pods: 3
      node:
        cores: 10
        memory: 16
        node_capacity_type: spot

  cluster_definition1: &cluster_definition1 # for reusing same definition and reduce repetitive cluster configs
    advance_config:
      <<: *advance_config1
    head_node:
      <<: *head_node_config1
    is_job_cluster: true
    runtime: *runtime
    start_cluster: false
    tags:
    - environment_tag
    terminate_after_minutes: *terminate_after_minutes
    worker_group:
    - *worker_node_config1 #reusing worker_node_config
    - max_pods: 1 #adding new worker node config
      min_pods: 1
      node:
        cores: 1
        memory: 2
        node_capacity_type: ondemand

  cluster_definition2: &cluster_definition2
    advance_config:
      <<: *advance_config2 #reusing advance_config
    head_node:
      <<: *head_node_config2  #reusing head_node_config
    is_job_cluster: true
    runtime: *runtime
    start_cluster: false
    tags:
    - environment_tag
    terminate_after_minutes: *terminate_after_minutes
    worker_group:
    - *worker_node_config2
    - max_pods: 1
      min_pods: 1
      node:
        cores: 4
        memory: 16
        node_capacity_type: spot

clusters:
- name: job_cluster_1
  <<: *cluster_definition1 #reusing variable cluster definition

- name: job_cluster_2
  <<: *cluster_definition2
  advance_config:
    <<: *advance_config3 #overriding advance_config for this cluster

darwin:
  base_dir: 'test.user@example.com/project/codespace'
  created_by: 'test.user@example.com'
  env: 'uat'


workflows: #can also define variables for workflows, and use them for repetitive workflows fields
- workflow_name: workflow_name_1
  display_name: display_name_1
  workflow_status: "active"
  description: description
  max_concurrent_runs: 0
  notify_on: slack_channel_name
  retries: 0
  schedule: "0 9 * * *"
  tags:
    - tag1
  start_date: "2025-03-26T11:07:22"
  end_date: "string"
  callback_urls: []
  event_types: []
  tenant: "d11"
  expected_run_duration: 60
  queue_enabled: false
  notification_preference:
    on_start: false
    on_fail: true
    on_success: false
    on_skip: false
  tasks: #can also define variables for tasks, and use them for repetitive task fields
    - task_name: start_task
      <<: *basic-packages #use packages defined in variables
      cluster_id: job_cluster_1  # using cluster defined above
      cluster_type: job
      dependent_libraries:
        - 'test_library'
      depends_on: []
      dynamic_artifact: true
      input_parameters: {}
      retries: 1
      source: folder/task_start.ipynb
      source_type: workspace
      timeout: 7200
      ha_config:
        enable_ha: true
        replication_factor: 3
        cluster_expiration_time: 86400
    - task_name: check_status_task
      cluster_id: job_cluster_2
      cluster_type: job
      <<: *basic-packages  # Merge the basic packages here
      packages:  # override packages for this task
        - pypi: {'package': 'pandas==1.24.0', 'path': ''}
      dependent_libraries:
        - 'test_library'
      depends_on:
        - start_task
      dynamic_artifact: true
      input_parameters: {}
      retries: 1
      source: folder_2/task_start_2.ipynb
      source_type: workspace
      timeout: 7200
