---
# Global configuration
global:
  # Use local Kind storage (hostPath) vs real K8s storage (PVCs)
  # Defaults to true here so it works out-of-the-box for local testing.
  # When deploying to production, this value should be overridden to false.
  local-k8s: true

replicaCount: 1
logLevel: info
name: ml-serve-demo
cluster_name: darwin-prod-eks-cluster
ui:
  color: "#34577c"
  message: ''
  logo: ''
pdb:
  enabled: true
  minAvailable: 1
envs:
  ENV: darwin-prod
  SERVICE_NAME: ml-serve-demo
  TEAM_SUFFIX: ""
  VPC_SUFFIX: ""

# Model caching with flexible strategies
# - emptydir: Per-pod download via init container (simple, no shared storage needed)
# - pvc: Shared cache via pre-deploy job (efficient for multiple replicas)
# - auto: Automatically choose based on model size (requires storage_strategy.py)
modelCache:
  enabled: false
  strategy: emptydir
  cachePath: /model-cache
  
  # Image used for pre-deployment job and cleanup job
  # MUST include: mlflow and kubernetes Python libraries
  # The default serve-md-runtime image includes both
  downloaderImage: localhost:5000/serve-md-runtime:latest
  
  # PVC for shared model cache (must be RWX-capable)
  # When strategy is "pvc", a PVC will be automatically created per-namespace
  pvcName: ml-model-cache  # Name of the PVC (created automatically when using pvc strategy)
  size: 50Gi               # Size of the PVC when auto-created
  # storageClassName: ""   # Optional: specify storage class (uses cluster default if not set)
  
  # Pre-deployment job resource limits
  # These should be sized based on expected model sizes:
  # - Small models (<1GB):     current defaults are fine
  # - Medium models (1-5GB):   current defaults sufficient
  # - Large models (5-20GB):   current defaults sufficient  
  # - Very large models (>20GB): increase memory to 8-16Gi
  # 
  # Memory is needed for:
  # - MLflow client overhead (~100MB)
  # - Download buffers (~500MB)
  # - Staging directory (full model size)
  # - OS page cache for better I/O
  #
  # CPU affects:
  # - Network I/O throughput
  # - File system operations
  # - Decompression if artifacts are compressed
  resources:
    requests:
      cpu: 1         # Higher for faster network/disk I/O
      memory: 2Gi    # Sufficient for most models
    limits:
      cpu: 4         # Allow burst for network/disk I/O
      memory: 8Gi    # Sufficient for models up to ~20GB
    # NOTE: For very large models (>20GB), further increase:
    # - memory: 16Gi (to handle staging + decompression)
    # - cpu: 4-8 (for faster I/O operations)
  maxRetries: 5
  backoffSeconds: 10

image:
  repository: localhost:5000/darwin
  tag: ml_serve_demo
  pullPolicy: Always
service:
  enabled: true
  type: ClusterIP
  httpPort: 8000
  externalPort: 80
tls:
  enabled: false
hpa:
  enabled: true
  maxReplicas: 3
  cpu: 60
  memory:
    target: Utilization
    value: 80
    unit: Mi
livenessProbe:
  initialDelaySeconds: 20
  periodSeconds: 5
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 5
  httpGet:
    path: healthcheck
    port: 8000
readinessProbe:
  initialDelaySeconds: 20
  periodSeconds: 5
  timeoutSeconds: 1
  successThreshold: 1
  failureThreshold: 5
  httpGet:
    path: healthcheck
    port: 8000
serviceAccount:
  enabled: false
serviceMonitor:
  enabled: false
  interval: 15s
ingressInt:
  namespace: darwin
  ingressClass: alb
  annotations:
    external-dns.alpha.kubernetes.io/hostname: ml-serve-demo.darwin.dream11-k8s.local
  enabled: true
  healthcheckPath: "/healthcheck"
  tags: "Environment=prod, Service=ml-serve-demo, squad=darwin, provisioned-by-user=harsh.a@dream11.com,
    environment_name=prod, component_name=ml-serve-demo-alb, service_name=ml-serve-demo,
    resource_type=alb, component_type=application, org_name=dream11"
  albLogs:
    enabled: false
    bucket: d11-logs
    prefix: alb-logs
  path: "/*"
  pathType: ImplementationSpecific
  hosts: 
  - localhost
  tls: []
ingressExt:
  enabled: false
resources:
  limits:
    cpu: 2
    memory: 4G
  requests:
    cpu: 2
    memory: 4G
nodeSelector:
  karpenter.sh/capacity-type: spot
  serve: "true"
tolerations: []
affinity: {}
podAnnotations: {}
command: ""
configServer:
  enabled: false
  consulToken: null
  vaultToken: null
labels: []
org: dream11
flagger:
  metrics:
  - name: sidecar-error-rate-measure
    thresholdRange:
      max: 1
    interval: 1m
  enabled: false
  type: canary
  interval: 1m
  threshold: 2
  skipAnalysis: false
  maxWeight: 60
  stepWeight: 20
  iterations: 2
  maxSurge: 10%
  maxUnavailable: 1
  slackChannel: darwin-serve-alerts
