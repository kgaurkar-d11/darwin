name: Darwin SDK - Integration Tests

on:
  workflow_dispatch:  # Allow manual triggering
  # pull_request:
  #   types: [opened, synchronize, reopened]
  #   branches:
  #     - main
  #     - develop
  #   paths:
  #     - 'darwin-sdk/**'
  #     - 'darwin-compute/**'
  #     - 'darwin-cluster-manager/**'
  #     - 'init.sh'
  #     - 'setup.sh'
  #     - 'start.sh'
  #     - 'services.yaml'
  #     - '.github/workflows/darwin-sdk-integration.yml'

jobs:
  integration-test:
    name: Integration Test with Ray Cluster
    runs-on: [self-hosted, Linux, X64, darwin]
    timeout-minutes: 60
    defaults:
      run:
        working-directory: .
    env:
      KUBECONFIG: ./.setup/kindkubeconfig.yaml
      CONFIG_ENV: ./.setup/config.env

    steps:
      - name: Runner identity
        run: |
          echo "Runner: $RUNNER_NAME | OS: $RUNNER_OS | Arch: $RUNNER_ARCH"
          hostname

      - name: Cleanup previous artifacts
        run: |
          echo "üßπ Cleaning up previous run artifacts..."
          if command -v kind >/dev/null 2>&1; then
            kind delete cluster --name kind 2>/dev/null || true
          fi
          WORKSPACE_DIR="${{ github.workspace }}"
          if [ -d "$WORKSPACE_DIR/kind" ]; then
            sudo chmod -R u+w "$WORKSPACE_DIR/kind" 2>/dev/null || true
            sudo rm -rf "$WORKSPACE_DIR/kind/shared-storage" 2>/dev/null || true
          fi
          
          # Completely clean workspace to fix sparse checkout issues from previous runs
          echo "üóëÔ∏è Removing entire workspace for fresh checkout..."
          cd /
          sudo rm -rf "$WORKSPACE_DIR" || true
          mkdir -p "$WORKSPACE_DIR"

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Docker
        uses: docker/setup-buildx-action@v3

      - name: Install prerequisites
        run: |
          # Install kubectl
          KUBECTL_VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
          echo "Installing kubectl $KUBECTL_VERSION"
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          # Verify the download is actually a binary (not an error page)
          if ! file kubectl | grep -q "ELF\|executable"; then
            echo "‚ùå Downloaded file is not a binary, retrying with direct URL..."
            rm -f kubectl
            curl -LO "https://storage.googleapis.com/kubernetes-release/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          fi
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

          # Install Helm
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
          helm version

          # Install Kind
          curl -Lo /tmp/kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64
          chmod +x /tmp/kind && sudo mv /tmp/kind /usr/local/bin/kind
          kind version

          # Install yq
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          yq --version

      - name: Configure services (init.sh)
        run: |
          echo "üìã Running init.sh to configure services..."
          rm -f .setup/enabled-services.yaml

          # Answers (13 prompts):
          # 1: compute (y) - required for darwin-sdk
          # 2-8: workspace, feature_store, mlflow, serve, catalog, chronos, workflow (all n)
          # 9: confirm (y)
          # 10: darwin-sdk-runtime (y) - required for darwin-sdk
          # 11-12: ray:2.37.0, ray:2.53.0 (all n)
          # 13: darwin-cli (n)
          printf "y\nn\nn\nn\nn\nn\nn\nn\ny\ny\nn\nn\nn\n" | ./init.sh --dev-mode || {
            echo "‚ùå init.sh failed"
            [ -f .setup/enabled-services.yaml ] && cat .setup/enabled-services.yaml
            exit 1
          }

          echo "‚úÖ Configuration created:"
          cat .setup/enabled-services.yaml

      - name: Build and setup cluster (setup.sh)
        run: |
          echo "üî® Running setup.sh..."
          export ENV=local
          ./setup.sh -y -d

          if [ -f $CONFIG_ENV ]; then
            echo "‚úÖ config.env created:"
            cat $CONFIG_ENV
          else
            echo "‚ùå config.env not found"
            exit 1
          fi

      - name: Verify cluster is ready
        run: |
          echo "üîç Verifying cluster status..."
          
          if [ ! -f "$KUBECONFIG" ]; then
            echo "‚ùå KUBECONFIG not found at $KUBECONFIG, cluster may not be ready"
            exit 1
          fi
          
          # Wait for cluster to be ready
          max_attempts=10
          attempt=0
          while [ $attempt -lt $max_attempts ]; do
            if kubectl get nodes >/dev/null 2>&1; then
              echo "‚úÖ Cluster is accessible"
              kubectl get nodes
              break
            fi
            echo "Attempt $((attempt+1))/$max_attempts: Waiting for cluster..."
            sleep 5
            attempt=$((attempt+1))
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo "‚ùå Cluster is not accessible"
            exit 1
          fi

      - name: Deploy to Kubernetes (start.sh)
        run: |
          echo "üöÄ Running start.sh..."

          # Background status monitor
          (
            while true; do
              sleep 30
              echo "‚è≥ [$(date +%H:%M:%S)] Status:"
              helm list -n darwin 2>/dev/null | head -3 || true
              kubectl get pods -n darwin --no-headers 2>/dev/null | head -5 | awk '{print "  " $1 ": " $3}' || true
            done
          ) &
          MONITOR_PID=$!

          set +e
          ./start.sh 2>&1 | tee /tmp/start.sh.log
          EXIT_CODE=${PIPESTATUS[0]}
          set -e

          kill $MONITOR_PID 2>/dev/null || true

          if [ $EXIT_CODE -ne 0 ]; then
            echo "‚ùå start.sh failed with exit code $EXIT_CODE"
            echo "=== Diagnostics ==="
            helm list -n darwin || true
            kubectl get pods -n darwin || true
            kubectl get events -n darwin --sort-by='.lastTimestamp' | tail -20 || true
            for pod in $(kubectl get pods -n darwin -o jsonpath='{.items[?(@.status.phase!="Running")].metadata.name}' 2>/dev/null); do
              echo "--- Pod: $pod ---"
              kubectl describe pod -n darwin "$pod" 2>/dev/null | tail -30 || true
              kubectl logs -n darwin "$pod" --tail=30 2>/dev/null || true
            done
            exit 1
          fi

          echo "‚úÖ Deployment completed"

      - name: Wait for pods to be ready
        run: |
          echo "‚è≥ Waiting for darwin-compute and darwin-cluster-manager pods..."

          # Wait for darwin-compute
          echo "Waiting for darwin-compute..."
          if ! kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=compute -n darwin --timeout=300s 2>/dev/null; then
            COMPUTE_POD=$(kubectl get pods -n darwin --no-headers 2>/dev/null | grep -E "darwin-compute" | head -1 | awk '{print $1}')
            if [ -n "$COMPUTE_POD" ]; then
              kubectl wait --for=condition=ready pod/$COMPUTE_POD -n darwin --timeout=300s || {
                echo "‚ùå darwin-compute pod did not become ready"
                kubectl describe pod/$COMPUTE_POD -n darwin || true
                exit 1
              }
            else
              echo "‚ùå Could not find darwin-compute pod"
              kubectl get pods -n darwin || true
              exit 1
            fi
          fi
          echo "‚úÖ darwin-compute pod is ready"

          # Wait for darwin-cluster-manager
          echo "Waiting for darwin-cluster-manager..."
          if ! kubectl wait --for=condition=ready pod -l app.kubernetes.io/component=cluster-manager -n darwin --timeout=300s 2>/dev/null; then
            DCM_POD=$(kubectl get pods -n darwin --no-headers 2>/dev/null | grep -E "darwin-cluster-manager" | head -1 | awk '{print $1}')
            if [ -n "$DCM_POD" ]; then
              kubectl wait --for=condition=ready pod/$DCM_POD -n darwin --timeout=300s || {
                echo "‚ùå darwin-cluster-manager pod did not become ready"
                kubectl describe pod/$DCM_POD -n darwin || true
                exit 1
              }
            else
              echo "‚ùå Could not find darwin-cluster-manager pod"
              kubectl get pods -n darwin || true
              exit 1
            fi
          fi
          echo "‚úÖ darwin-cluster-manager pod is ready"

          kubectl get pods -n darwin | grep -E "compute|cluster-manager"

      - name: Test compute service health
        run: |
          echo "üè• Testing compute service health before cluster creation..."
          
          for i in $(seq 1 30); do
            if curl -f -s http://localhost/compute/health > /dev/null 2>&1; then
              echo "‚úÖ Compute service is healthy"
              curl -s http://localhost/compute/health | python3 -m json.tool
              break
            fi
            [ $i -eq 30 ] && { echo "‚ùå Compute service not accessible"; exit 1; }
            echo "Attempt $i/30: Waiting for compute service..."
            sleep 2
          done

      - name: Create Ray test cluster via API
        id: create_cluster
        run: |
          source $CONFIG_ENV
          
          echo "üöÄ Creating Ray test cluster via Compute API..."
          
          # Create cluster request payload
          CLUSTER_REQUEST=$(cat <<'EOF'
          {
            "cluster_name": "sdk-integration-test",
            "tags": ["local", "ci-test"],
            "runtime": "1.0",
            "inactive_time": 60,
            "start_cluster": true,
            "head_node_config": {
              "cores": 2,
              "memory": 4
            },
            "worker_node_configs": [
              {
                "cores_per_pods": 1,
                "memory_per_pods": 2,
                "min_pods": 2,
                "max_pods": 2
              }
            ],
            "user": "ci-test@darwin.local"
          }
          EOF
          )
          
          echo "Request payload:"
          echo "$CLUSTER_REQUEST" | python3 -m json.tool
          
          # Create cluster via API
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            -H "Content-Type: application/json" \
            -d "$CLUSTER_REQUEST" \
            http://localhost/compute/cluster)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          BODY=$(echo "$RESPONSE" | sed '$d')
          
          echo "Response (HTTP $HTTP_CODE):"
          echo "$BODY" | python3 -m json.tool || echo "$BODY"
          
          if [ "$HTTP_CODE" != "200" ]; then
            echo "‚ùå Failed to create cluster (HTTP $HTTP_CODE)"
            exit 1
          fi
          
          # Extract cluster_id from response
          CLUSTER_ID=$(echo "$BODY" | python3 -c "import sys, json; print(json.load(sys.stdin)['data']['cluster_id'])")
          echo "‚úÖ Cluster created with ID: $CLUSTER_ID"
          
          # Save cluster_id for subsequent steps
          echo "CLUSTER_ID=$CLUSTER_ID" >> $GITHUB_ENV
          echo "cluster_id=$CLUSTER_ID" >> $GITHUB_OUTPUT

      - name: Wait for Ray cluster to be ready
        run: |
          source $CONFIG_ENV
          
          echo "‚è≥ Waiting for cluster $CLUSTER_ID to become ACTIVE..."
          
          max_attempts=60
          attempt=0
          while [ $attempt -lt $max_attempts ]; do
            METADATA=$(curl -s http://localhost/compute/cluster/$CLUSTER_ID/metadata)
            STATUS=$(echo "$METADATA" | python3 -c "import sys, json; print(json.load(sys.stdin).get('data', {}).get('status', 'UNKNOWN'))" 2>/dev/null || echo "UNKNOWN")
            
            echo "Attempt $((attempt+1))/$max_attempts: Cluster status is $STATUS"
            
            if [ "$STATUS" = "active" ]; then
              echo "‚úÖ Cluster is active!"
              echo "$METADATA" | python3 -m json.tool
              break
            elif [ "$STATUS" = "inactive" ] || [ "$STATUS" = "error" ]; then
              echo "‚ùå Cluster creation failed with status: $STATUS"
              echo "$METADATA" | python3 -m json.tool
              exit 1
            fi
            
            sleep 10
            attempt=$((attempt+1))
          done
          
          if [ $attempt -eq $max_attempts ]; then
            echo "‚ùå Timeout waiting for cluster to become active"
            exit 1
          fi
          
          # Also verify pods are running
          echo ""
          echo "Verifying Ray pods..."
          kubectl get pods -n ray -l ray.io/cluster=$CLUSTER_ID || kubectl get pods -A | grep -i $CLUSTER_ID || true

      - name: Setup port-forward for Ray cluster
        run: |
          source $CONFIG_ENV
          
          echo "üîå Setting up port-forward for cluster $CLUSTER_ID..."
          
          # Get Ray head pod name - try different label patterns
          RAY_HEAD_POD=$(kubectl get pods -n ray -l ray.io/node-type=head -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          
          if [ -z "$RAY_HEAD_POD" ]; then
            echo "Trying alternative pod discovery..."
            RAY_HEAD_POD=$(kubectl get pods -A -l ray.io/node-type=head -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          fi
          
          if [ -z "$RAY_HEAD_POD" ]; then
            echo "Searching for head pod by cluster ID..."
            RAY_HEAD_POD=$(kubectl get pods -n ray --no-headers 2>/dev/null | grep -E "head|$CLUSTER_ID" | head -1 | awk '{print $1}' || true)
          fi
          
          if [ -z "$RAY_HEAD_POD" ]; then
            echo "‚ùå Could not find Ray head pod"
            kubectl get pods -A
            exit 1
          fi
          
          echo "Ray head pod: $RAY_HEAD_POD"
          
          # Determine namespace
          RAY_NAMESPACE=$(kubectl get pods -A -o jsonpath="{.items[?(@.metadata.name=='$RAY_HEAD_POD')].metadata.namespace}" 2>/dev/null || echo "ray")
          echo "Ray namespace: $RAY_NAMESPACE"
          
          # Port forward Ray client port (10001)
          kubectl port-forward -n $RAY_NAMESPACE pod/$RAY_HEAD_POD 10001:10001 &
          
          # Wait for port-forward to be ready
          sleep 5
          
          echo "‚úÖ Port-forward established"

      - name: Install Darwin SDK
        working-directory: darwin-sdk/darwin
        run: |
          # Set version for build
          echo "SPARK_VERSION=3.5.0" > version.txt
          echo "BUILD_VERSION=1.0.0-ci" >> version.txt
          
          # Install in development mode
          pip install -e .
          pip install pytest pytest-mock
          
          # Verify installation
          python -c "import darwin; print('‚úÖ Darwin SDK installed')"

      - name: Run Ray cluster connectivity test
        working-directory: darwin-sdk/darwin
        env:
          RAY_ADDRESS: ray://localhost:10001
          ENV: CI
          DARWIN_COMPUTE_URL: http://localhost/compute
        run: |
          echo "üîå Testing Ray cluster connectivity..."
          echo "Using CLUSTER_ID: $CLUSTER_ID"
          python cluster_test.py

      - name: Run integration tests
        working-directory: darwin-sdk/darwin
        env:
          RAY_ADDRESS: ray://localhost:10001
          ENV: CI
          DARWIN_COMPUTE_URL: http://localhost/compute
          AWS_ACCESS_KEY_ID: test
          AWS_SECRET_ACCESS_KEY: test
          AWS_DEFAULT_REGION: us-east-1
          AWS_EC2_METADATA_DISABLED: true
        run: |
          echo "üß™ Running Darwin SDK integration tests..."
          echo "Using CLUSTER_ID: $CLUSTER_ID"
          pytest tests/test_integration.py \
            -v \
            --tb=short \
            --junitxml=integration-test-results.xml

      - name: Test darwin.init() functionality
        working-directory: darwin-sdk/darwin
        env:
          RAY_ADDRESS: ray://localhost:10001
          ENV: CI
          DARWIN_COMPUTE_URL: http://localhost/compute
          AWS_ACCESS_KEY_ID: test
          AWS_SECRET_ACCESS_KEY: test
          AWS_DEFAULT_REGION: us-east-1
          AWS_EC2_METADATA_DISABLED: true
        run: |
          echo "üß™ Testing darwin.init() with mocked compute service..."
          echo "Using CLUSTER_ID: $CLUSTER_ID"
          python -c "
          import os
          from unittest.mock import MagicMock, patch
          import darwin
          
          cluster_id = os.environ.get('CLUSTER_ID', 'sdk-integration-test')
          
          # Mock compute service response
          mock_response = {
              'data': {
                  'cluster_id': cluster_id,
                  'name': 'sdk-integration-test',
                  'status': 'RUNNING',
                  'has_ondemand_worker_group': True,
                  'spark_config': {
                      'spark.app.name': 'darwin-ci-test',
                      'spark.master': 'local[2]',
                      'spark.executor.cores': '1',
                      'spark.executor.memory': '1G',
                      'spark.driver.memory': '1G',
                      'spark.darwin.workingDir': '/tmp/darwin',
                      'spark.darwin.enableRemoteShuffle': 'false',
                      'spark.darwin.dynamicAllocation.enabled': 'false',
                      'spark.darwin.loggingLevel': 'WARN'
                  }
              }
          }
          
          # Create mock
          mock_data = MagicMock()
          mock_data.cluster_id = mock_response['data']['cluster_id']
          mock_data.name = mock_response['data']['name']
          mock_data.status = mock_response['data']['status']
          mock_data.has_ondemand_worker_group = mock_response['data']['has_ondemand_worker_group']
          mock_data.spark_config = mock_response['data']['spark_config']
          
          mock_cluster_response = MagicMock()
          mock_cluster_response.data = mock_data
          
          mock_service = MagicMock()
          mock_service.get_compute_metadata.return_value = mock_cluster_response
          
          with patch('darwin.compute.service.ComputeService', return_value=mock_service):
              # Test darwin.init_spark()
              spark = darwin.init_spark()
              print('‚úÖ darwin.init_spark() successful')
              
              # Test basic Spark operation
              df = spark.range(100)
              count = df.count()
              assert count == 100, f'Expected 100, got {count}'
              print(f'‚úÖ Spark DataFrame operation successful (count={count})')
              
              # Stop Spark
              darwin.stop_spark()
              print('‚úÖ darwin.stop_spark() successful')
          
          print('‚úÖ All darwin.init() tests passed!')
          "

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: darwin-sdk/darwin/integration-test-results.xml
          retention-days: 7

      - name: Show Ray cluster logs on failure
        if: failure()
        run: |
          source $CONFIG_ENV || true
          echo "=== Ray Head Pod Logs ==="
          kubectl logs -n ray -l ray.io/node-type=head --tail=100 || true
          echo ""
          echo "=== Ray Worker Pod Logs ==="
          kubectl logs -n ray -l ray.io/node-type=worker --tail=100 || true
          echo ""
          echo "=== Cluster Metadata ==="
          curl -s http://localhost/compute/cluster/$CLUSTER_ID/metadata 2>/dev/null | python3 -m json.tool || true

      - name: Show platform status on failure
        if: failure()
        run: |
          source $CONFIG_ENV || true
          echo "=== Pods Status ==="
          kubectl get pods -A || true
          echo ""
          echo "=== Services Status ==="
          kubectl get services -A || true
          echo ""
          echo "=== Recent Events ==="
          kubectl get events -A --sort-by='.lastTimestamp' | tail -30 || true
          echo ""
          echo "=== Compute Service Logs ==="
          kubectl logs -n darwin -l app=compute --tail=100 || true
          echo ""
          echo "=== Cluster Manager Logs ==="
          kubectl logs -n darwin -l app=cluster-manager --tail=100 || true

      - name: Integration test summary
        if: always()
        run: |
          echo "## Darwin SDK Integration Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Cluster ID:** \`$CLUSTER_ID\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f darwin-sdk/darwin/integration-test-results.xml ]; then
            echo "‚úÖ Integration tests completed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå Integration tests failed or did not run" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Ray Cluster Status" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          source $CONFIG_ENV || true
          kubectl get pods -n ray 2>&1 || echo "Could not get Ray cluster status"
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

      - name: Cleanup
        if: always()
        run: |
          echo "üßπ Cleaning up..."
          echo "Current disk usage:"
          df -h
          echo ""
          echo "Stopping Ray test cluster via API..."
          if [ -n "$CLUSTER_ID" ]; then
            curl -s -X POST \
              -H "msd-user: {\"email\": \"ci-test@darwin.local\"}" \
              "http://localhost/compute/cluster/stop-cluster/$CLUSTER_ID" || true
            echo "Cluster stop request sent for: $CLUSTER_ID"
            
            # Wait briefly for cluster to start stopping
            sleep 5
            
            # Check final status
            curl -s "http://localhost/compute/cluster/$CLUSTER_ID/metadata" 2>/dev/null | python3 -m json.tool || true
          else
            echo "No CLUSTER_ID found, skipping API cleanup"
          fi
          echo ""
          echo "Cleaning up kind cluster and port-forwards..."
          pkill -f "kubectl port-forward" || true
          kind delete cluster --name kind 2>/dev/null || true
          echo ""
          echo "Cleaning up Docker resources..."
          docker image prune -af --filter "label=maintainer=darwin" || true
          docker system prune -f || true
          docker volume prune -f || true
          echo ""
          echo "Cleaning up apt cache..."
          sudo apt-get clean || true
          sudo rm -rf /var/lib/apt/lists/* || true
          echo ""
          echo "Disk usage after cleanup:"
          df -h
